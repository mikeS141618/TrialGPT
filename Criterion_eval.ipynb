{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d73de630-162c-4e86-a049-9e4090a6b59a",
   "metadata": {},
   "source": [
    "The notebook is a comprehensive data analysis script that processes and merges multiple datasets related to patient trials, preparing the data for further modeling or evaluation. It begins by reading a Parquet file 'TrialGPT-Criterion-Annotations/data/train-00000-of-00001.parquet' into a DataFrame df_parquet, and a TSV file 'TrialGPT/dataset/sigir/qrels/test.tsv' into df_tsv. To facilitate merging, the TSV file's columns are renamed to match those in df_parquet (renaming 'query-id' to 'patient_id' and 'corpus-id' to 'trial_id'). The notebook performs data cleaning by trimming whitespace from string columns and examines unique values and distributions within columns to understand the data's structure. An inner merge on ['patient_id', 'trial_id'] results in df_joined, and the script identifies unmatched records to assess the completeness and integrity of the merge.\n",
    "\n",
    "Subsequently, the notebook reads a JSON Lines file 'TrialGPT/dataset/sigir/queries.jsonl', which contains patient query data, and filters it to include only those patient IDs present in df_joined. This filtered dataset is saved for future use. A further merge combines the filtered queries with df_joined, producing df_final, which consolidates patient information, trial data, and associated scores. The notebook conducts statistical analyses on df_final, such as calculating the distribution of matching scores, aggregating statistics on the number of trials per patient, and examining the distinct criteria texts associated with patients. Visualizations, including histograms and bar plots, depict distributions of trials per patient and criteria per patient, aiding in identifying patterns or anomalies in the data.\n",
    "\n",
    "In the later sections, the focus shifts to data sampling and preparation for detailed analysis. The script filters patients who have exactly two trials and a specific range of distinct criteria (between 15 and 25), randomly selecting five patients for an in-depth examination. Corresponding query data for these patients is saved to 'TrialGPT/dataset/sigir/mini_filtered_queries.jsonl'. Finally, the notebook reads and processes a JSON file containing trial rankings ('TrialGPT/results/trial_rankings/all_rankings.json'), flattens the nested JSON structure into a DataFrame, and provides summary statistics on the number of patients and trials. This prepares the data for subsequent analysis or modeling steps, ensuring that the dataset is well-understood and ready for advanced processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8131a5-a6f2-4d97-8866-f3958873da29",
   "metadata": {},
   "source": [
    "## Summary of the current work:\n",
    "\n",
    "This notebook focuses on analyzing and comparing different eligibility assessments for clinical trials, involving patient data, trial information, and various eligibility labels. The main steps and analyses include:\n",
    "\n",
    "1. Data Loading and Preprocessing:\n",
    "   - Loaded multiple datasets: trial annotations, patient queries, trial corpus, and matching results.\n",
    "   - Performed data cleaning, merging, and filtering operations to create comprehensive datasets.\n",
    "\n",
    "2. Data Analysis:\n",
    "   - Analyzed distributions of trials per patient and criteria per patient.\n",
    "   - Examined the structure and content of various datasets, including patient summaries and trial criteria.\n",
    "\n",
    "3. Eligibility Label Comparison:\n",
    "   - Created a merged dataset combining information from multiple sources.\n",
    "   - Compared three types of eligibility labels: expert-annotated, GPT-4 generated, and algorithm-predicted.\n",
    "   - Used confusion matrices, F1 scores, and classification reports to evaluate the performance of different labeling methods.\n",
    "\n",
    "4. Visualization:\n",
    "   - Created various plots to visualize the distribution of scores, criteria, and performance metrics.\n",
    "\n",
    "5. Detailed Comparison:\n",
    "   - Generated tables highlighting discrepancies between different eligibility assessments.\n",
    "   - Analyzed cases where predictions differed from expert annotations or GPT-4 assessments.\n",
    "\n",
    "Contrast with the initial summary:\n",
    "\n",
    "The initial summary at the top of the notebook described a data processing pipeline focused on merging and preparing datasets for modeling. It mentioned:\n",
    "- Reading and processing Parquet and TSV files.\n",
    "- Merging datasets based on patient and trial IDs.\n",
    "- Basic statistical analyses and visualizations of the merged data.\n",
    "- Sampling and filtering operations to create a subset of data for detailed analysis.\n",
    "\n",
    "The current work goes significantly beyond this initial scope:\n",
    "1. Depth of Analysis: While the initial summary focused on data preparation, the current work delves deep into comparing and analyzing different eligibility assessments.\n",
    "2. Multiple Data Sources: The current analysis incorporates more data sources, including GPT-4 generated labels and algorithm predictions, which weren't mentioned in the initial summary.\n",
    "3. Advanced Analytical Techniques: The notebook employs more sophisticated analytical methods, such as confusion matrices, F1 scores, and detailed classification reports.\n",
    "4. Focus on Eligibility Assessment: The current work specifically targets the comparison of different methods for determining trial eligibility, which wasn't a focus in the initial summary.\n",
    "5. Visualization and Interpretation: The notebook includes more advanced visualizations and interpretations of the results, providing deeper insights into the performance of different eligibility assessment methods.\n",
    "\n",
    "In essence, while the initial summary described a data preparation process, the current work represents a comprehensive analysis of clinical trial eligibility assessments, comparing expert annotations with AI-generated predictions and exploring the nuances and discrepancies between these different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc689cf-e9a2-4224-8d74-118129ba9f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "\n",
    "# Set pandas display options \n",
    "pd.set_option('display.max_rows', 1)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9775c8-d319-4116-9599-ee4452a81c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_sort_imports(import_list):\n",
    "    # Split each import statement into lines\n",
    "    imports = import_list.split('\\n')\n",
    "    \n",
    "    # Remove empty lines, strip whitespace, and remove duplicates\n",
    "    unique_imports = list(dict.fromkeys(line.strip() for line in imports if line.strip()))\n",
    "    \n",
    "    # Sort imports by length, then alphabetically for imports of the same length\n",
    "    sorted_imports = sorted(unique_imports, key=lambda x: (len(x), x))\n",
    "    \n",
    "    return sorted_imports\n",
    "\n",
    "# Example usage:\n",
    "imports = \"\"\"\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "\n",
    "cleaned_sorted_imports = clean_and_sort_imports(imports)\n",
    "\n",
    "# Print the cleaned and sorted imports\n",
    "for imp in cleaned_sorted_imports:\n",
    "    print(imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccfa219-6894-4ab1-9ae2-a87076e1b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet file\n",
    "parquet_file = '../TrialGPT-Criterion-Annotations/data/train-00000-of-00001.parquet'\n",
    "df_parquet = pd.read_parquet(parquet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681e9162-4d85-4907-842d-62f8ff1079eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not pre downloaded use this below, or download from hf \n",
    "# df_parquet = pd.read_parquet(\"hf://datasets/ncbi/TrialGPT-Criterion-Annotations/data/train-00000-of-00001.parquet\")\n",
    "# https://huggingface.co/datasets/ncbi/TrialGPT-Criterion-Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd9a35-cecc-4c27-8773-80e898d51e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print both DataFrames\n",
    "print(\"Parquet DataFrame:\")\n",
    "df_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44c2c8b-f61d-40d1-b264-e9ec78f5a37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TSV file\n",
    "tsv_file = 'dataset/sigir/qrels/test.tsv'\n",
    "df_tsv = pd.read_csv(tsv_file, sep='\\t', names=['query-id', 'corpus-id', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb03d212-b88e-4cc4-bd6d-b1718a3c2d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display info about the DataFrame\n",
    "print(\"\\nDataFrame Info:\")\n",
    "print(df_tsv.info())\n",
    "\n",
    "# Display basic statistics of the DataFrame\n",
    "print(\"\\nDataFrame Description:\")\n",
    "print(df_tsv.describe())\n",
    "\n",
    "# Display value counts for each column\n",
    "for column in df_tsv.columns:\n",
    "    print(f\"\\nValue counts for {column}:\")\n",
    "    print(df_tsv[column].value_counts().head())\n",
    "\n",
    "# # Optional: Save to CSV for easy viewing in spreadsheet software\n",
    "# df_tsv.to_csv('test_tsv_data.csv', index=False)\n",
    "# print(\"\\nDataFrame saved to 'test_tsv_data.csv'\")# Display the first few rows of the DataFrame\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"First few rows of the DataFrame:\")\n",
    "df_tsv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1967460-59e8-4d03-a42f-80d39462d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the results\n",
    "column_stats = {}\n",
    "\n",
    "# Loop through each column\n",
    "for column in df_parquet.columns:\n",
    "    # Get unique values\n",
    "    unique_values = df_parquet[column].unique()\n",
    "    \n",
    "    # Convert to list and sort (if possible)\n",
    "    try:\n",
    "        unique_list = sorted(unique_values.tolist())\n",
    "    except TypeError:\n",
    "        # If sorting fails (e.g., for mixed types), just convert to list without sorting\n",
    "        unique_list = unique_values.tolist()\n",
    "    \n",
    "    # Store in dictionary\n",
    "    column_stats[f\"{column}_list\"] = unique_list\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"{column}:\")\n",
    "    print(f\"  Number of unique values: {len(unique_list)}\")\n",
    "    print(f\"  First few unique values: {unique_list[:5]}\")  # Show first 5 values\n",
    "    print()\n",
    "\n",
    "# Now column_stats dictionary contains lists for each column\n",
    "# You can access them like this: column_stats['patient_id_list']\n",
    "\n",
    "# # Optional: Save to file\n",
    "# import json\n",
    "# with open('column_stats.json', 'w') as f:\n",
    "#     json.dump(column_stats, f, indent=2)\n",
    "# print(\"Results saved to column_stats.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995e89c2-9d38-45bf-a0aa-d2a739956510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in df_tsv to match df_parquet\n",
    "# This step ensures that the column names used for merging are consistent across both dataframes\n",
    "df_tsv = df_tsv.rename(columns={'query-id': 'patient_id', 'corpus-id': 'trial_id'})\n",
    "\n",
    "# Perform the join\n",
    "# We're using a right join here, which keeps all rows from df_parquet and matching rows from df_tsv\n",
    "# This is important because it ensures we don't lose any data from df_parquet, contains our main dataset\n",
    "df_joined = pd.merge(df_tsv, df_parquet, on=['patient_id', 'trial_id'], how='right') \n",
    "\n",
    "# Display info about the joined DataFrame\n",
    "# This gives us an overview of the resulting dataframe, including column names and data types\n",
    "print(\"\\nJoined DataFrame Info:\")\n",
    "print(df_joined.info())\n",
    "\n",
    "# Display basic statistics of the joined DataFrame\n",
    "# This provides summary statistics for numerical columns, helping us understand the data distribution\n",
    "print(\"\\nJoined DataFrame Description:\")\n",
    "print(df_joined.describe())\n",
    "\n",
    "# Display the number of rows in each DataFrame\n",
    "# This helps us verify if we've lost or gained any rows during the merge operation\n",
    "print(f\"\\nNumber of rows in df_tsv: {len(df_tsv)}\")\n",
    "print(f\"Number of rows in df_parquet: {len(df_parquet)}\")\n",
    "print(f\"Number of rows in joined DataFrame: {len(df_joined)}\")\n",
    "\n",
    "# Extract unique patient IDs from the joined DataFrame\n",
    "# This helps us understand how many distinct patients are in our dataset\n",
    "distinct_patient_ids = df_joined['patient_id'].unique()\n",
    "\n",
    "# Calculate and print the number of distinct patients\n",
    "# This gives us a quick count of how many unique patients we're dealing with\n",
    "num_distinct_patients = len(distinct_patient_ids)\n",
    "print(f\"Number of distinct patient_ids: {num_distinct_patients}\")\n",
    "\n",
    "# Convert unique patient IDs to a list and print them\n",
    "# This allows us to see all unique patient IDs, which can be useful for debugging or further analysis\n",
    "distinct_patient_ids_list = distinct_patient_ids.tolist()\n",
    "print(\"\\nDistinct patient_ids as a list:\")\n",
    "print(distinct_patient_ids_list)\n",
    "\n",
    "# Display the entire joined DataFrame\n",
    "# This allows us to see the structure and content of our merged data\n",
    "df_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe2d9d-a53d-4cc7-938f-3c31e9295311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSONL file from the dataset\n",
    "# This file is part of the TrialGPT dataset and contains query information\n",
    "# The 'queries.jsonl' file likely includes patient information\n",
    "jsonl_file = 'dataset/sigir/queries.jsonl'\n",
    "queries = []\n",
    "\n",
    "# Open the file and read it line by line\n",
    "with open(jsonl_file, 'r') as file:\n",
    "    for line in file:\n",
    "        # Parse each line as a JSON object and append it to the queries list\n",
    "        # This approach is memory-efficient for large files as it processes one line at a time\n",
    "        queries.append(json.loads(line))\n",
    "\n",
    "# Convert the list of dictionaries (parsed JSON objects) to a pandas DataFrame\n",
    "# This transformation allows for easier data manipulation and analysis using pandas functions\n",
    "df_queries = pd.DataFrame(queries)\n",
    "\n",
    "# Display the entire DataFrame\n",
    "# This allows us to inspect the structure and content of the queries data from the dataset\n",
    "df_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a0263-0277-4ce9-90df-c289bc20cae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df_queries to keep only the _ids present in df_joined's patient_id\n",
    "# This step ensures we only work with queries that have corresponding data in our joined dataset\n",
    "df_queries_filtered = df_queries[df_queries['_id'].isin(df_joined['patient_id'])]\n",
    "\n",
    "# Display information about the filtering process\n",
    "# This helps us understand how many queries were kept and how many were removed\n",
    "print(f\"Total number of queries: {len(df_queries)}\")\n",
    "print(f\"Number of queries after filtering: {len(df_queries_filtered)}\")\n",
    "print(f\"Number of queries removed: {len(df_queries) - len(df_queries_filtered)}\")\n",
    "\n",
    "# Display the first few rows of the filtered queries DataFrame\n",
    "# This allows us to inspect the structure and content of the filtered data\n",
    "print(\"\\nFirst few rows of filtered queries:\")\n",
    "print(df_queries_filtered.head())\n",
    "\n",
    "\n",
    "# Total number of queries: 59\n",
    "# Number of queries after filtering: 52\n",
    "# # Number of queries removed: 7\n",
    "# What this means:\n",
    "\n",
    "#     The original queries.jsonl file contained information for 59 patients.\n",
    "#     After filtering, we're left with 52 patients that have corresponding data in the df_joined DataFrame.\n",
    "#     7 patients were removed during the filtering process.\n",
    "\n",
    "# A \"removed query\" in this context refers to a patient entry that exists in the queries.jsonl file (which is part of the TrialGPT dataset for testing) but does not have a corresponding entry in the TrialGPT-Criterion-Annotations set (represented by df_joined).\n",
    "\n",
    "# These removed queries are significant because:\n",
    "\n",
    "#     They represent patients that are part of the testing dataset but not part of the annotation set.\n",
    "#     This discrepancy could be intentional, possibly to test the model's performance on unseen data.\n",
    "#     It highlights the difference between the full testing dataset and the subset that has detailed annotations.\n",
    "\n",
    "# The filtering step ensures that we're working with a consistent set of patients across our different data sources. However, it's important to note that by removing these 7 queries, we might be excluding some test cases that were intended to evaluate the model's performance on patients not seen in the training data.\n",
    "\n",
    "# This observation is crucial for understanding the scope of our analysis and any potential limitations in our evaluation process. It also underscores the importance of carefully considering how we handle data that appears in one dataset but not another when working with multiple related datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532d3ebe-3f12-41bf-a598-b87501268974",
   "metadata": {},
   "source": [
    "# Importance of this step:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c41553c-1483-42a0-938c-062876dbf30f",
   "metadata": {},
   "source": [
    "### don't run with min_to_run openAI branch need full queries.jsonl for qrels = GenericDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d275ad43-30ae-47a9-b375-5df01bb5295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save filtered queries to a new JSONL file\n",
    "# This step creates a new dataset that only includes patients with corresponding annotations\n",
    "output_file = 'dataset/sigir/filtered_queries.jsonl'\n",
    "with open(output_file, 'w') as file:\n",
    "    for _, row in df_queries_filtered.iterrows():\n",
    "        json.dump(row.to_dict(), file)\n",
    "        file.write('\\n')\n",
    "print(f\"\\nFiltered queries saved to {output_file}\")\n",
    "\n",
    "# # Optional: Save filtered queries to CSV for easy viewing\n",
    "# df_queries_filtered.to_csv('filtered_queries.csv', index=False)\n",
    "# print(\"Filtered queries also saved to filtered_queries.csv\")\n",
    "\n",
    "# Importance of this step:\n",
    "#     Data Consistency: By saving only the filtered queries, we ensure that our dataset aligns perfectly with the annotation data. This consistency is crucial for accurate model evaluation.\n",
    "#     Resource Efficiency: As you correctly pointed out, we're avoiding unnecessary processing of data for which we don't have annotations. This saves computational resources and time.\n",
    "#     Replication Focus: By using just the annotation data, we're focusing on replicating the results based on the subset of data that has been expertly annotated. This allows for a more direct comparison with the reported results.\n",
    "#     Reducing Noise: Excluding patients without annotations eliminates potential noise or inconsistencies that could arise from trying to process or predict outcomes for patients lacking expert-labeled data.\n",
    "# Why this approach is safe:\n",
    "#     Data Integrity: We're not altering or manipulating the content of the queries, just selecting a subset. This preserves the integrity of the individual patient data.\n",
    "#     Traceability: By creating a new file, we maintain a clear record of which queries were used in our analysis, making our process transparent and reproducible.\n",
    "#     Original Data Preservation: The original dataset remains untouched, allowing us to always refer back to it if needed.\n",
    "#     Controlled Environment: This approach creates a controlled dataset that matches exactly with our annotation data, reducing variables that could affect our ability to replicate the reported results.\n",
    "#     Alignment with Best Practices: In machine learning and data science, it's common and often recommended to work with a clearly defined and consistent dataset, especially when trying to replicate results.\n",
    "# By taking this approach, you're creating a more controlled environment for your analysis, which is crucial when attempting to replicate and verify reported results. It allows you to focus on the core task of eligibility assessment based on expert-annotated data, without introducing potential confounding factors from unannotated cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa689d-a28b-450c-a656-5696c237770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of unique _ids in the filtered queries\n",
    "# This helps us understand how many distinct patients we have in our filtered query dataset\n",
    "print(f\"\\nNumber of unique _ids in filtered queries: {df_queries_filtered['_id'].nunique()}\")\n",
    "\n",
    "# Display the number of unique patient_ids in df_joined\n",
    "# This shows how many distinct patients we have in our joined dataset\n",
    "print(f\"Number of unique patient_ids in df_joined: {df_joined['patient_id'].nunique()}\")\n",
    "\n",
    "# Find patient_ids in df_joined that are not in df_queries_filtered\n",
    "# This identifies any patients in the joined dataset that don't have corresponding queries\n",
    "extra_ids = set(df_joined['patient_id']) - set(df_queries_filtered['_id'])\n",
    "print(f\"\\nNumber of patient_ids in df_joined not present in filtered queries: {len(extra_ids)}\")\n",
    "\n",
    "# Check for any _ids in filtered queries not present in df_joined's patient_id\n",
    "# This identifies any queries that don't have corresponding patient data in the joined dataset\n",
    "missing_ids = set(df_queries_filtered['_id']) - set(df_joined['patient_id'])\n",
    "print(f\"\\nNumber of _ids in filtered queries not present in df_joined: {len(missing_ids)}\")\n",
    "if missing_ids:\n",
    "    print(\"Sample of missing _ids:\", list(missing_ids)[:5])\n",
    "\n",
    "# If there are extra IDs in df_joined, print them and their corresponding rows\n",
    "if extra_ids:\n",
    "    print(\"Extra patient_ids in df_joined:\")\n",
    "    for id in extra_ids:\n",
    "        print(id)\n",
    "    \n",
    "    # Retrieve and print the full rows for these extra IDs in df_joined\n",
    "    extra_rows = df_joined[df_joined['patient_id'].isin(extra_ids)]\n",
    "    print(\"\\nFull rows for extra IDs in df_joined:\")\n",
    "    print(extra_rows)\n",
    "else:\n",
    "    print(\"No extra patient_ids found in df_joined.\")\n",
    "\n",
    "# Double-check the counts to ensure consistency\n",
    "print(f\"\\nConfirming counts:\")\n",
    "print(f\"df_queries_filtered '_id' count: {df_queries_filtered['_id'].count()}\")\n",
    "print(f\"df_joined 'patient_id' count: {df_joined['patient_id'].count()}\")\n",
    "\n",
    "\n",
    "# Number of unique _ids in filtered queries: 52\n",
    "# Number of unique patient_ids in df_joined: 53\n",
    "\n",
    "# Number of patient_ids in df_joined not present in filtered queries: 1\n",
    "\n",
    "# Number of _ids in filtered queries not present in df_joined: 0\n",
    "# Extra patient_ids in df_joined:\n",
    "# sigir-201426\n",
    "# Confirming counts:\n",
    "# df_queries_filtered '_id' count: 52\n",
    "# df_joined 'patient_id' count: 1015\n",
    "# ```\n",
    "\n",
    "# These counts indeed indicate that we have one more annotated patient in the TrialGPT-Criterion-Annotations dataset than we do in the filtered queries.jsonl file. Here's the detailed explanation:\n",
    "\n",
    "# 1. Unique Patients Discrepancy:\n",
    "#    - We have 52 unique patients in the filtered queries dataset.\n",
    "#    - We have 53 unique patients in the joined dataset (df_joined).\n",
    "#    - This immediately shows us that there's one extra patient in the annotations.\n",
    "\n",
    "# 2. Extra Patient Identification:\n",
    "#    - The code identified one patient ID (sigir-201426) that exists in df_joined but not in the filtered queries.\n",
    "#    - This confirms that we have an additional annotated patient that doesn't have a corresponding query in the queries.jsonl file.\n",
    "\n",
    "# 3. No Missing Queries:\n",
    "#    - There are 0 query IDs that exist in the filtered queries but not in df_joined.\n",
    "#    - This means all the patients in our queries dataset have corresponding annotations.\n",
    "\n",
    "# 4. Total Annotation Count:\n",
    "#    - The df_joined 'patient_id' count is 1015, which likely represents the total number of annotations (not unique patients).\n",
    "#    - This suggests that on average, we have about 19 annotations per patient (1015 / 53 ≈ 19).\n",
    "\n",
    "# 5. Consistency in Filtered Queries:\n",
    "#    - The filtered queries dataset has 52 entries, matching the number of unique IDs.\n",
    "#    - This indicates that each patient in the queries dataset has exactly one query entry.\n",
    "\n",
    "# Implications:\n",
    "\n",
    "# 1. Data Completeness: We have more annotated data than query data.\n",
    "\n",
    "# 2. Potential for Analysis: The extra patient (sigir-201426) in the annotations dataset provides an opportunity to examine why this discrepancy exists. It could be due to:\n",
    "#    - An intentional addition for testing purposes.\n",
    "#    - A data collection or processing error.\n",
    "#    - A patient that was annotated but whose query was later removed for some reason.\n",
    "\n",
    "# 3. Data Integrity: The fact that all filtered queries have corresponding annotations is a positive sign for data integrity.\n",
    "\n",
    "# 4. Comprehensive Annotations: With an average of 19 annotations per patient, we have a rich dataset for analysis, likely covering multiple criteria or trials per patient.\n",
    "\n",
    "# This discrepancy, while small, is important to note for several reasons:\n",
    "# - It ensures transparency in your data processing pipeline.\n",
    "# - It may affect how you calculate certain statistics or perform your analysis.\n",
    "# - It provides an opportunity to investigate why this extra patient exists in the annotations and whether it should be included or excluded in further analysis.\n",
    "\n",
    "# Understanding these nuances in your dataset is crucial for accurate analysis and interpretation of results, especially when trying to replicate or validate previous findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b3de50-59d9-4ef1-9b90-2c71c70683c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the '_id' column in df_queries_filtered to 'patient_id' for consistency in the join operation\n",
    "df_queries_filtered = df_queries_filtered.rename(columns={'_id': 'patient_id'})\n",
    "\n",
    "# Perform an inner join between df_queries_filtered and df_joined on the 'patient_id' column\n",
    "# This ensures we only keep rows where patient_id exists in both dataframes\n",
    "df_final = pd.merge(df_queries_filtered, df_joined, on='patient_id', how='inner')\n",
    "\n",
    "# Display basic information about the joined DataFrame\n",
    "print(f\"Number of rows in final joined DataFrame: {len(df_final)}\")\n",
    "print(f\"Number of unique patients: {df_final['patient_id'].nunique()}\")\n",
    "print(f\"Number of unique trials: {df_final['trial_id'].nunique()}\")\n",
    "\n",
    "# Count the occurrences of each score value\n",
    "score_counts = df_final['score'].value_counts().sort_index()\n",
    "print(\"\\nCount of scores:\")\n",
    "print(score_counts)\n",
    "\n",
    "# Calculate the percentage distribution of scores\n",
    "score_percentages = (score_counts / len(df_final)) * 100\n",
    "print(\"\\nPercentage of each score:\")\n",
    "print(score_percentages)\n",
    "\n",
    "# Compute and display basic statistical measures of the scores\n",
    "print(\"\\nBasic statistics of scores:\")\n",
    "print(df_final['score'].describe())\n",
    "\n",
    "# Visualize the distribution of scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "score_counts.plot(kind='bar')\n",
    "plt.title('Distribution of Scores')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('score_distribution.png')  # Uncomment to save the plot\n",
    "print(\"\\nScore distribution plot saved as 'score_distribution.png'\")\n",
    "\n",
    "# Display the entire final joined DataFrame\n",
    "df_final\n",
    "\n",
    "\n",
    "# Number of rows in final joined DataFrame: 997\n",
    "# Number of unique patients: 52\n",
    "# Number of unique trials: 101\n",
    "\n",
    "# Count of scores:\n",
    "# score\n",
    "# 1    598\n",
    "# 2    399\n",
    "# Name: count, dtype: int64\n",
    "\n",
    "# Percentage of each score:\n",
    "# score\n",
    "# 1    59.97994\n",
    "# 2    40.02006\n",
    "# Name: count, dtype: float64\n",
    "\n",
    "# Basic statistics of scores:\n",
    "# count     997\n",
    "# unique      2\n",
    "# top         1\n",
    "# freq      598\n",
    "# Name: score, dtype: object\n",
    "\n",
    "\n",
    "# 1. 997 rows: This number represents the total patient-trial combinations in our final dataset. It's less than the 1015 we saw earlier because:\n",
    "#    - We filtered out the one patient (sigir-201426) that was in the annotations but not in the queries.\n",
    "#    - The inner join operation only kept rows where patient IDs existed in both the filtered queries and the joined dataset.\n",
    "\n",
    "# 2. 52 unique patients: This matches exactly with the number of unique patients we had in our filtered queries dataset. It confirms that we've successfully maintained all the patients from our queries in the final dataset.\n",
    "\n",
    "# 3. 101 unique trials: This shows how many different clinical trials are represented in our dataset. Each patient might be evaluated for multiple trials.\n",
    "\n",
    "# 4. Score distribution:\n",
    "#    - We only have two score values: 1 and 2.\n",
    "#    - Score 1 appears 598 times (59.98% of the cases)\n",
    "#    - Score 2 appears 399 times (40.02% of the cases)\n",
    "\n",
    "# This distribution tells us that our dataset is slightly imbalanced, with more instances of score 1 than score 2. In the context of clinical trial eligibility, these scores likely represent different levels of eligibility or relevance.\n",
    "\n",
    "# The fact that we have 997 rows (patient-trial combinations) for 52 patients means that, on average, each patient is evaluated for about 19 trials (997 / 52 ≈ 19.17). This aligns with our earlier observation about the number of annotations per patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472f0eaf-0f14-4ea2-92f7-cbe281fcc435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique trials for each patient\n",
    "trials_per_patient = df_final.groupby('patient_id')['trial_id'].nunique().sort_values(ascending=False)\n",
    "\n",
    "# Display the distribution of trials per patient\n",
    "print(\"\\nDistribution of trials per patient:\")\n",
    "print(trials_per_patient.value_counts().sort_index())\n",
    "\n",
    "# Identify patients with the most and least trials\n",
    "max_trials = trials_per_patient.max()\n",
    "min_trials = trials_per_patient.min()\n",
    "\n",
    "# Display patients with the maximum number of trials\n",
    "print(f\"\\nPatients with the most trials ({max_trials}):\")\n",
    "print(trials_per_patient[trials_per_patient == max_trials].index.tolist())\n",
    "\n",
    "# Display patients with the minimum number of trials\n",
    "print(f\"\\nPatients with the least trials ({min_trials}):\")\n",
    "print(trials_per_patient[trials_per_patient == min_trials].index.tolist())\n",
    "\n",
    "# Add the number of trials for each patient back to df_final\n",
    "df_final['num_trials'] = df_final['patient_id'].map(trials_per_patient)\n",
    "\n",
    "# Count the number of distinct criteria texts for each patient\n",
    "criteria_per_patient = df_final.groupby('patient_id')['criterion_text'].nunique().sort_values(ascending=False)\n",
    "\n",
    "# Display basic statistics of the number of distinct criteria per patient\n",
    "print(\"Statistics of distinct criteria per patient:\")\n",
    "print(criteria_per_patient.describe())\n",
    "\n",
    "# Display the distribution of distinct criteria per patient\n",
    "print(\"\\nDistribution of distinct criteria per patient:\")\n",
    "print(criteria_per_patient.value_counts().sort_index())\n",
    "\n",
    "# Calculate and display the percentage distribution of distinct criteria per patient\n",
    "percentage_distribution = (criteria_per_patient.value_counts().sort_index() / len(criteria_per_patient)) * 100\n",
    "print(\"\\nPercentage distribution of distinct criteria per patient:\")\n",
    "print(percentage_distribution)\n",
    "\n",
    "# Visualize the distribution of distinct criteria per patient\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(criteria_per_patient, kde=True, bins=30)\n",
    "plt.title('Distribution of Distinct Criteria per Patient')\n",
    "plt.xlabel('Number of Distinct Criteria')\n",
    "plt.ylabel('Number of Patients')\n",
    "# plt.savefig('criteria_per_patient_distribution.png')\n",
    "# plt.close()\n",
    "print(\"\\nDistribution plot saved as 'criteria_per_patient_distribution.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f663543-d6e3-4535-bab4-d5c135217929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    'num_criteria': criteria_per_patient.value_counts().sort_index().index,\n",
    "    'num_patients': criteria_per_patient.value_counts().sort_index().values,\n",
    "    'percentage': percentage_distribution.values\n",
    "})\n",
    "\n",
    "# Display the summary DataFrame\n",
    "print(\"\\nSummary of distinct criteria per patient:\")\n",
    "print(summary_df)\n",
    "\n",
    "# # Save the summary to a CSV file\n",
    "# summary_df.to_csv('criteria_per_patient_summary.csv', index=False)\n",
    "# print(\"\\nSummary saved to 'criteria_per_patient_summary.csv'\")\n",
    "\n",
    "# Identify patients with the most and least distinct criteria\n",
    "max_criteria = criteria_per_patient.max()\n",
    "min_criteria = criteria_per_patient.min()\n",
    "\n",
    "print(f\"\\nPatients with the most distinct criteria ({max_criteria}):\")\n",
    "print(criteria_per_patient[criteria_per_patient == max_criteria].index.tolist())\n",
    "\n",
    "print(f\"\\nPatients with the least distinct criteria ({min_criteria}):\")\n",
    "print(criteria_per_patient[criteria_per_patient == min_criteria].index.tolist())\n",
    "\n",
    "# Optional: If you want to add this information back to df_final\n",
    "df_final['num_distinct_criteria'] = df_final['patient_id'].map(criteria_per_patient)\n",
    "\n",
    "# Display the first few rows of the updated df_final\n",
    "print(\"\\nFirst few rows of df_final with num_distinct_criteria added:\")\n",
    "print(df_final.head())\n",
    "\n",
    "# # Save the updated df_final\n",
    "# df_final.to_csv('df_final_with_criteria_counts.csv', index=False)\n",
    "# print(\"\\nUpdated df_final saved to 'df_final_with_criteria_counts.csv'\")\n",
    "\n",
    "# Additional analysis: Most common criteria texts\n",
    "top_criteria = df_final['criterion_text'].value_counts().head(10)\n",
    "print(\"\\nTop 10 most common criteria texts:\")\n",
    "print(top_criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c66f15-c251-4b4b-99c6-ad35f5f0ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d447be-d7e4-4e9c-8467-5e8badad5f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to select patients with exactly 2 trials and between 15-25 distinct criteria\n",
    "filtered_df = df_final[\n",
    "    (df_final['num_trials'] == 2) & \n",
    "    (df_final['num_distinct_criteria'].between(15, 25))\n",
    "].drop_duplicates('patient_id')\n",
    "\n",
    "# Count how many patients meet these criteria\n",
    "num_matching_patients = filtered_df['patient_id'].nunique()\n",
    "print(f\"Number of patients meeting the criteria: {num_matching_patients}\")\n",
    "\n",
    "sampsto = 5  # Number of patients to sample\n",
    "\n",
    "# If we have at least 'sampsto' patients meeting the criteria, randomly select that many\n",
    "if num_matching_patients >= sampsto:\n",
    "    # Randomly select 'sampsto' patients\n",
    "    selected_patients = random.sample(filtered_df['patient_id'].unique().tolist(), sampsto)\n",
    "    \n",
    "    # Print details of the selected patients\n",
    "    print(\"\\nRandomly selected patient_ids:\")\n",
    "    for i, patient_id in enumerate(selected_patients, 1):\n",
    "        patient_data = filtered_df[filtered_df['patient_id'] == patient_id].iloc[0]\n",
    "        print(f\"{i}. patient_id: {patient_id}\")\n",
    "        print(f\"   num_trials: {patient_data['num_trials']}\")\n",
    "        print(f\"   num_distinct_criteria: {patient_data['num_distinct_criteria']}\")\n",
    "        print()\n",
    "\n",
    "    # Create a new DataFrame with all data for these selected patients\n",
    "    selected_df = df_final[df_final['patient_id'].isin(selected_patients)]\n",
    "\n",
    "else:\n",
    "    print(\"Not enough patients meet the criteria. Please adjust the filter conditions.\")\n",
    "\n",
    "# Filter df_queries to keep only the _ids present in selected_df's patient_id\n",
    "df_queries_filtered = df_queries[df_queries['_id'].isin(selected_df['patient_id'])]\n",
    "\n",
    "# Display information about the filtering process\n",
    "print(f\"Total number of queries: {len(df_queries)}\")\n",
    "print(f\"Number of queries after filtering: {len(df_queries_filtered)}\")\n",
    "print(f\"Number of queries removed: {len(df_queries) - len(df_queries_filtered)}\")\n",
    "\n",
    "# Save filtered queries to a new JSONL file\n",
    "output_file = 'dataset/sigir/mini_filtered_queries.jsonl'\n",
    "with open(output_file, 'w') as file:\n",
    "    for _, row in df_queries_filtered.iterrows():\n",
    "        json.dump(row.to_dict(), file)\n",
    "        file.write('\\n')\n",
    "print(f\"\\nFiltered queries saved to {output_file}\")\n",
    "\n",
    "# Verify the number of queries in the saved file\n",
    "print(f\"\\nNumber of queries in the JSONL file: {sum(1 for line in open(output_file))}\")\n",
    "\n",
    "\n",
    "# 1. Patient Selection:\n",
    "#    - Filters patients based on specific criteria (2 trials and 15-25 distinct criteria).\n",
    "#    - Randomly selects a subset of these patients for further analysis.\n",
    "\n",
    "# 2. Data Extraction:\n",
    "#    - Creates a new DataFrame (selected_df) containing all data for the selected patients.\n",
    "\n",
    "# 3. Query Filtering:\n",
    "#    - Filters the original queries dataset to include only the selected patients.\n",
    "\n",
    "# 4. Data Saving:\n",
    "#    - Saves the filtered queries to a new JSONL file.\n",
    "\n",
    "# 5. Verification:\n",
    "#    - Checks the number of queries in the saved file to ensure data integrity.\n",
    "\n",
    "# This process is crucial for several reasons:\n",
    "# - It creates a manageable subset of data for detailed analysis or testing.\n",
    "# - The selection criteria ensure that the chosen patients have a similar level of complexity (in terms of trials and criteria), which can be important for fair comparisons or model testing.\n",
    "# - Saving this subset as a separate file allows for easier replication of analyses or tests.\n",
    "# - The random selection helps to avoid bias that might come from manually selecting patients.\n",
    "\n",
    "# The verification steps throughout (counting patients, queries, etc.) are essential for ensuring that the data manipulation processes have worked as expected and that the resulting dataset matches our intentions.\n",
    "\n",
    "# This approach of creating a smaller, well-defined subset of the data is often used in machine learning and data analysis to create test sets, perform detailed case studies, or to create manageable datasets for initial model development and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd1b2b-ae7f-4f42-9493-22fb02921350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the corpus.jsonl file\n",
    "# corpus.jsonl contains detailed information about clinical trials\n",
    "# Each line in this file is a JSON object representing a single trial\n",
    "# The structure includes fields like '_id' (trial ID), 'title', 'text', and 'metadata'\n",
    "corpus_file = 'dataset/sigir/corpus.jsonl'\n",
    "\n",
    "# Use pandas to read the JSONL file\n",
    "# The 'lines=True' parameter tells pandas to read the file as JSON Lines format\n",
    "# where each line is a separate JSON object\n",
    "df_corpus = pd.read_json(corpus_file, lines=True)\n",
    "\n",
    "# Display the entire DataFrame\n",
    "df_corpus\n",
    "\n",
    "# Note on corpus.jsonl structure:\n",
    "# Each entry in corpus.jsonl looks like this:\n",
    "# {\n",
    "#   \"_id\": \"NCT01520155\",\n",
    "#   \"title\": \"CArdiovascular Risk Assessment STudy in Lupus Erythemathodes (CASTLE)\",\n",
    "#   \"text\": \"Summary: The key of this prospective study is to identify...\",\n",
    "#   \"metadata\": {\n",
    "#     \"brief_title\": \"CArdiovascular Risk Assessment STudy in Lupus Erythemathodes (CASTLE)\",\n",
    "#     \"phase\": \"\",\n",
    "#     \"drugs\": \"\",\n",
    "#     \"drugs_list\": [],\n",
    "#     \"diseases\": \"['Systemic Lupus Erythematosus']\",\n",
    "#     \"diseases_list\": [\"Systemic Lupus Erythematosus\"],\n",
    "#     \"enrollment\": \"90.0\",\n",
    "#     \"inclusion_criteria\": \"inclusion criteria: \\n\\n Patients with systemic Lupus erythematosus \\n\\n \",\n",
    "#     \"exclusion_criteria\": \": \\n\\n Patients without systemic Lupus erythematosus\",\n",
    "#     \"brief_summary\": \"The key of this prospective study is to identify...\"\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c0509e-c8ec-45fa-8f99-a88d1e5febf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter the corpus to include only the trials present in df_final\n",
    "# This step ensures we only keep trials that are relevant to our analysis\n",
    "df_corpus_filtered = df_corpus[df_corpus['_id'].isin(df_final['trial_id'])]\n",
    "\n",
    "# Display the filtered corpus DataFrame\n",
    "df_corpus_filtered\n",
    "\n",
    "# Save the filtered corpus to 'corpus_mini.jsonl'\n",
    "# This creates a smaller, focused dataset for further analysis\n",
    "output_file = 'dataset/sigir/corpus_mini.jsonl'\n",
    "df_corpus_filtered.to_json(output_file, orient='records', lines=True)\n",
    "\n",
    "print(f\"Filtered corpus saved to {output_file}\")\n",
    "\n",
    "\n",
    "# Why this filtering is safe and prevents unnecessary calculation:\n",
    "\n",
    "# 1. Data Relevance:\n",
    "#    - By filtering the corpus to include only trials present in df_final, we ensure that we're working with only the relevant trials for our analysis.\n",
    "#    - This aligns the corpus data with the TrialGPT-Criterion-Annotations dataset, maintaining consistency across our datasets.\n",
    "\n",
    "# 2. Computational Efficiency:\n",
    "#    - Reducing the size of the corpus eliminates the need to process trials that are not part of our analysis scope.\n",
    "#    - This can significantly reduce computation time and resource usage in subsequent steps, especially if the original corpus is large.\n",
    "\n",
    "# 3. Focus on Annotated Data:\n",
    "#    - By keeping only the trials that correspond to the TrialGPT-Criterion-Annotations dataset, we ensure that all trials in our filtered corpus have associated annotations.\n",
    "#    - This alignment is crucial for accurate analysis and model training, as we have expert-annotated data for these trials.\n",
    "\n",
    "# 4. Preventing Noise:\n",
    "#    - Removing trials not present in the annotation dataset eliminates potential noise or irrelevant data that could skew our analysis or model performance.\n",
    "\n",
    "# 5. Consistency in Replication:\n",
    "#    - This filtering step ensures that we're working with the exact set of trials that were part of the original study's annotated dataset.\n",
    "#    - This is crucial for accurately replicating and validating the results of the original study.\n",
    "\n",
    "# 6. Storage Efficiency:\n",
    "#    - Saving a smaller, filtered corpus (corpus_mini.jsonl) reduces storage requirements and makes the dataset more manageable for sharing or further processing.\n",
    "\n",
    "# 7. Ethical Considerations:\n",
    "#    - By focusing only on trials with associated annotations, we're ensuring that all data used in our analysis has been properly vetted and annotated by experts.\n",
    "\n",
    "# 8. Reproducibility:\n",
    "#    - This filtered corpus, along with the other filtered datasets, creates a well-defined, reproducible dataset for your analysis.\n",
    "\n",
    "# In essence, this filtering step is not just safe but also highly beneficial. It streamlines your dataset, focuses on the most relevant information, and aligns perfectly with the annotated data you're working with. \n",
    "# This approach ensures that your analysis is based on a consistent, well-defined set of trials, which is crucial for the integrity and reliability of your results in the clinical trial eligibility assessment task."
   ]
  },
  {
   "cell_type": "raw",
   "id": "28206a12-3a28-4e64-a72f-5de67abed8c6",
   "metadata": {},
   "source": [
    "# filter pre made retrieved_trials to just those in annotation "
   ]
  },
  {
   "cell_type": "raw",
   "id": "58e727b2-af91-49ff-9a9c-ae68751a4f64",
   "metadata": {},
   "source": [
    "# Read the JSON file containing pre-compiled retrieved trials\n",
    "# This file is part of the original repo and represents their data\n",
    "with open('dataset/sigir/retrieved_trials.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Create a set of filtered NCTIDs from our previously filtered corpus\n",
    "# This set will be used to filter the retrieved trials\n",
    "filtered_nctids = set(df_corpus_filtered['_id'].tolist())\n",
    "\n",
    "# Define a function to filter trials based on the filtered NCTIDs\n",
    "def filter_trials(trials):\n",
    "    return [trial for trial in trials if trial['NCTID'] in filtered_nctids]\n",
    "\n",
    "# Filter the data while maintaining the original structure\n",
    "filtered_data = []\n",
    "for entry in data:\n",
    "    filtered_entry = entry.copy()\n",
    "    for key in ['0', '1', '2']:  # These keys likely represent different retrieval methods or rankings\n",
    "        if key in filtered_entry:\n",
    "            filtered_entry[key] = filter_trials(filtered_entry[key])\n",
    "    \n",
    "    # Keep the entry even if all lists are empty to maintain patient records\n",
    "    filtered_data.append(filtered_entry)\n",
    "\n",
    "# Save the filtered data to a new JSON file\n",
    "with open('dataset/sigir/retrieved_trials_mini.json', 'w') as file:\n",
    "    json.dump(filtered_data, file, indent=2)\n",
    "\n",
    "# Define a function to flatten the hierarchical data structure for easier processing\n",
    "def flatten_data(entry):\n",
    "    flattened = []\n",
    "    for key in ['0', '1', '2']:\n",
    "        if key in entry and isinstance(entry[key], list):\n",
    "            for item in entry[key]:\n",
    "                item['patient_id'] = entry['patient_id']\n",
    "                item['patient'] = entry['patient']\n",
    "                flattened.append(item)\n",
    "    return flattened\n",
    "\n",
    "# Flatten the filtered data and convert it to a DataFrame for further analysis\n",
    "flattened_data = [item for entry in filtered_data for item in flatten_data(entry)]\n",
    "df_filtered = pd.DataFrame(flattened_data)\n",
    "df_filtered\n",
    "\n",
    "# This code block is crucial for several reasons:\n",
    "\n",
    "# 1. Data Alignment: It aligns the pre-compiled retrieved trials from the original repository with our filtered dataset, ensuring we're working with a consistent set of trials.\n",
    "\n",
    "# 2. Focused Analysis: By filtering the retrieved trials to include only those with annotations, we're creating a direct comparison set that should match their test set. This is essential for validating their results and understanding their methodology.\n",
    "\n",
    "# 3. Structural Preservation: The code maintains the original structure of the data (with keys '0', '1', '2'), which likely represent different retrieval methods or ranking systems used in the original study.\n",
    "\n",
    "# 4. Comprehensive Comparison: By keeping entries even if all lists are empty, we maintain a record of all patients, allowing us to identify any discrepancies or gaps in the retrieval process.\n",
    "\n",
    "# 5. Data Transformation: The flattening process transforms the hierarchical JSON structure into a tabular format (DataFrame), making it easier to perform detailed analyses and comparisons.\n",
    "\n",
    "# 6. Reproducibility: This step is crucial for reproducing and validating the original study's results. By using their pre-compiled data and filtering it to match our annotated set, we're creating a fair basis for comparison.\n",
    "\n",
    "# 7. Insight into Methodology: This process might reveal insights into how the original study selected or processed their test set, which could be valuable for understanding their approach and results.\n",
    "\n",
    "# The significance of this step cannot be overstated. It allows us to:\n",
    "# - Directly compare our results with theirs using the same set of trials.\n",
    "# - Identify any discrepancies between the retrieved trials and the annotated set.\n",
    "# - Potentially uncover reasons for any performance differences we might observe.\n",
    "# - Ensure that our evaluation is based on the same foundation as the original study, making our comparisons and conclusions more robust and valid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c470e9-fd03-4606-8c2c-5675548da3a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for the new one filter an include all \n",
    "### after \n",
    "# Step 1 completed at 2025-01-26 20:54:55 (Duration: 197s)\n",
    "# Step 2 completed at 2025-01-26 20:58:10 (Duration: 5s)\n",
    "\n",
    "# This code is executed after Steps 1 and 2 in the new project(comprehensive overhaul)\n",
    "# It filters and processes the retrieved trials to match the annotated dataset\n",
    "\n",
    "# Read the JSON file containing retrieved trials from the new project\n",
    "with open('results/retrieved_trials.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Create a set of valid (patient_id, trial_id) combinations from df_joined\n",
    "# This ensures we only keep trials that have annotations in our dataset\n",
    "valid_combinations = set(df_joined[['patient_id', 'trial_id']].itertuples(index=False, name=None))\n",
    "\n",
    "# Function to filter trials based on patient_id and valid combinations\n",
    "def filter_trials(trials, patient_id):\n",
    "    return [trial for trial in trials if (patient_id, trial['NCTID']) in valid_combinations]\n",
    "\n",
    "# Filter the data while maintaining the original structure\n",
    "filtered_data = []\n",
    "for entry in data:\n",
    "    filtered_entry = entry.copy()\n",
    "    patient_id = entry['patient_id']\n",
    "    for key in ['0', '1', '2']:  # These likely represent different retrieval methods or rankings\n",
    "        if key in filtered_entry:\n",
    "            filtered_entry[key] = filter_trials(filtered_entry[key], patient_id)\n",
    "    \n",
    "    # Keep the entry even if all lists are empty to maintain patient records\n",
    "    filtered_data.append(filtered_entry)\n",
    "\n",
    "# Save the filtered data back to the same JSON file\n",
    "with open('results/retrieved_trials.json', 'w') as file:\n",
    "    json.dump(filtered_data, file, indent=2)\n",
    "\n",
    "# Function to flatten the hierarchical data structure for easier processing\n",
    "def flatten_data(entry):\n",
    "    flattened = []\n",
    "    for key in ['0', '1', '2']:\n",
    "        if key in entry and isinstance(entry[key], list):\n",
    "            for item in entry[key]:\n",
    "                item['patient_id'] = entry['patient_id']\n",
    "                item['patient'] = entry['patient']\n",
    "                flattened.append(item)\n",
    "    return flattened\n",
    "\n",
    "# Flatten the filtered data and convert it to a DataFrame for further analysis\n",
    "flattened_data = [item for entry in filtered_data for item in flatten_data(entry)]\n",
    "df_filtered = pd.DataFrame(flattened_data)\n",
    "\n",
    "# Print statistics about the filtered and flattened data\n",
    "print(f\"Total entries in filtered data: {len(filtered_data)}\")\n",
    "print(f\"Total flattened rows: {len(df_filtered)}\")\n",
    "print(f\"Unique patient_ids: {df_filtered['patient_id'].nunique()}\")\n",
    "print(f\"Unique NCTIDs: {df_filtered['NCTID'].nunique()}\")\n",
    "\n",
    "# Display the total number of rows in the filtered DataFrame\n",
    "len(df_filtered)\n",
    "\n",
    "# Note: This filtering allows for setting TOP_K=100 in processing\n",
    "# With 101 trials in corpus_mini.jsonl, this approach ensures all annotated cases are included\n",
    "# while significantly reducing the number of results to process (from potential 52k to a more manageable subset)\n",
    "\n",
    "# 1. Data Alignment: It aligns the retrieved trials from your new project with the annotated dataset, ensuring consistency and relevance.\n",
    "# 2. Efficiency: By filtering the retrieved trials to match only the annotated cases, you're significantly reducing the computational load for subsequent steps.\n",
    "# 3. Comprehensive Coverage: This approach ensures that all annotated cases are included in the analysis, providing a complete picture of the annotated dataset.\n",
    "# 4. Flexibility: Setting TOP_K=100 allows for a thorough analysis of retrieved trials per patient while keeping the dataset manageable.\n",
    "# 5. Reproducibility: This filtering step creates a well-defined subset of data that can be consistently used across different analyses or model runs.\n",
    "# 6. Performance Optimization: By reducing the dataset from a potential 52k results to a more focused subset, you're optimizing for both performance and relevance.\n",
    "# 7. Quality Control: This process helps in identifying any discrepancies between retrieved trials and annotated data, which is crucial for maintaining data quality.\n",
    "# 8. Scalability: While currently working with 101 trials, this approach scales well if you decide to expand the corpus in the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72a5ccb-11cb-4912-9fdf-424b7c370100",
   "metadata": {},
   "source": [
    "# up to here to filter dataset to just annotated data trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40481a95-f456-4a30-a605-97e88db2b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the results to be analyzed\n",
    "# Uncomment the appropriate line based on which results set you're working with\n",
    "# results_to_workwith = 'TrialGPT/results_gpt4o'\n",
    "# results_to_workwith = 'TrialGPT/results_gpt4o_perf103'\n",
    "results_to_workwith = 'results'\n",
    "# results_to_workwith = 'TrialGPT_working_raw/TrialGPT/results'\n",
    "\n",
    "# Read the JSON file containing all rankings data\n",
    "with open(f'{results_to_workwith}/trial_rankings/all_rankings.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Prepare a list to store the flattened data\n",
    "rows = []\n",
    "\n",
    "# Iterate through each patient in the data\n",
    "for patient_id, patient_data in data.items():\n",
    "    patient_summary = patient_data['patient_summary']\n",
    "    \n",
    "    # Iterate through each trial for this patient\n",
    "    for trial_id, trial_data in patient_data['trials'].items():\n",
    "        # Create a dictionary for each patient-trial combination\n",
    "        row = {\n",
    "            'patient_id': patient_id,\n",
    "            'patient_summary': patient_summary,\n",
    "            'trial_id': trial_id,\n",
    "            'matching_score': trial_data['matching_score'],\n",
    "            'agg_score': trial_data['agg_score'],\n",
    "            'trial_score': trial_data['trial_score'],\n",
    "            'qrels_score': trial_data['qrels_score'],\n",
    "            'brief_summary': trial_data['brief_summary'],\n",
    "            'relevance_explanation': trial_data['relevance_explanation'],\n",
    "            'eligibility_explanation': trial_data['eligibility_explanation']\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "# Create a DataFrame from the flattened data\n",
    "all_rankings = pd.DataFrame(rows)\n",
    "\n",
    "# Optional: Save the DataFrame to a CSV file\n",
    "# df.to_csv('flattened_trial_rankings.csv', index=False)\n",
    "# print(\"\\nDataFrame saved to 'flattened_trial_rankings.csv'\")\n",
    "\n",
    "# Print summary statistics about the DataFrame\n",
    "print(f\"\\nTotal number of rows: {len(all_rankings)}\")\n",
    "print(f\"Number of unique patients: {all_rankings['patient_id'].nunique()}\")\n",
    "print(f\"Number of unique trials: {all_rankings['trial_id'].nunique()}\")\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "all_rankings.head()\n",
    "\n",
    "# 1. Data Source Selection: It allows flexibility in choosing which set of results to analyze by uncommenting the appropriate `results_to_workwith` line.\n",
    "# 2. Data Loading: It reads the comprehensive rankings data from a JSON file, which likely contains the results of the trial matching process.\n",
    "# 3. Data Flattening: The nested JSON structure is flattened into a list of dictionaries, each representing a patient-trial combination. This makes the data more suitable for tabular analysis.\n",
    "# 4. Comprehensive Data Capture: For each patient-trial pair, it captures various scores (matching, aggregate, trial, and qrels scores) as well as summaries and explanations. This provides a rich dataset for analysis.\n",
    "# 5. DataFrame Creation: By converting the flattened data into a pandas DataFrame, it enables powerful data manipulation and analysis capabilities.\n",
    "# 7. Summary Statistics: It provides a quick overview of the dataset size and diversity by counting total rows, unique patients, and unique trials.\n",
    "# 8. Data Preview: The `head()` function allows for a quick inspection of the first few rows of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ae4f5-021f-499d-be24-1a82b986cc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261c7715-8276-4926-9a35-f1b22e5bb2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an inner join between df_tsv and all_rankings\n",
    "# df_tsv contains the ground truth labels from 'TrialGPT/dataset/sigir/qrels/test.tsv'\n",
    "# The labels in df_tsv represent:\n",
    "#   2: Eligible - The patient is eligible to enroll in the trial\n",
    "#   1: Excluded - The patient has the condition, but is ineligible due to exclusion criteria\n",
    "#   0: Not Relevant - The patient is not relevant for the trial in any way\n",
    "df_all_rankings_score = pd.merge(df_tsv, all_rankings, on=['patient_id', 'trial_id'], how='inner')\n",
    "\n",
    "# Display the number of rows in the resulting DataFrame\n",
    "len(df_all_rankings_score)\n",
    "\n",
    "# Display the entire DataFrame\n",
    "df_all_rankings_score\n",
    "\n",
    "\n",
    "# 1. Ground Truth Integration: \n",
    "#    - It merges the ground truth labels (df_tsv) with the model's rankings and scores (all_rankings).\n",
    "#    - This integration is essential for evaluating the performance of your trial matching system.\n",
    "\n",
    "# 2. Label Context:\n",
    "#    - The labels from df_tsv provide a standardized assessment of patient eligibility for each trial.\n",
    "#    - Understanding these labels (2, 1, 0) is crucial for interpreting the results and evaluating the model's performance.\n",
    "\n",
    "# 3. Data Alignment:\n",
    "#    - The inner join ensures that we're only working with patient-trial pairs that have both ground truth labels and model predictions.\n",
    "#    - This alignment is necessary for fair and accurate evaluation.\n",
    "\n",
    "# 4. Comprehensive Analysis:\n",
    "#    - The resulting DataFrame (df_all_rankings_score) combines ground truth labels with various scores and explanations from the model.\n",
    "#    - This allows for a multi-faceted analysis of the model's performance, including accuracy, explanation quality, and potential biases.\n",
    "\n",
    "# 5. Performance Metrics:\n",
    "#    - With both true labels and predicted scores in one DataFrame, it's easier to calculate performance metrics like accuracy, precision, recall, and F1-score.\n",
    "\n",
    "# 6. Error Analysis:\n",
    "#    - This merged dataset facilitates detailed error analysis by allowing easy comparison between true labels and model predictions.\n",
    "\n",
    "# 7. Model Insight:\n",
    "#    - The inclusion of model-generated explanations alongside true labels can provide insights into why the model made certain predictions, especially in cases of disagreement with ground truth.\n",
    "\n",
    "# 8. Dataset Verification:\n",
    "#    - Displaying the length of the resulting DataFrame helps verify that the merge operation didn't unexpectedly lose or duplicate data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a206debf-510a-4b6f-84dd-4c438c37419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'score' and 'qrels_score' to string type for consistent mapping\n",
    "df_all_rankings_score['score'] = df_all_rankings_score['score'].astype(str)\n",
    "df_all_rankings_score['qrels_score'] = df_all_rankings_score['qrels_score'].astype(str)\n",
    "\n",
    "# Define the mapping from numeric scores to categorical labels\n",
    "score_mapping = {\n",
    "    \"2\": \"Eligible\",\n",
    "    \"1\": \"Excluded\",\n",
    "    \"0\": \"Not Relevant\"\n",
    "}\n",
    "\n",
    "# Apply the mapping to create new columns with categorical labels\n",
    "df_all_rankings_score['score_mapped'] = df_all_rankings_score['score'].map(score_mapping)\n",
    "df_all_rankings_score['qrels_score_mapped'] = df_all_rankings_score['qrels_score'].map(score_mapping)\n",
    "\n",
    "# Define the order of labels for consistent analysis\n",
    "labels = [\"Not Relevant\", \"Excluded\", \"Eligible\"]  # Ensure correct order\n",
    "\n",
    "# Print unique values to verify successful mapping\n",
    "print(\"Unique values in 'score_mapped':\", df_all_rankings_score['score_mapped'].unique())\n",
    "print(\"Unique values in 'qrels_score_mapped':\", df_all_rankings_score['qrels_score_mapped'].unique())\n",
    "\n",
    "# Check for NaN values which could indicate mapping issues\n",
    "print(\"\\nNaN values in 'score_mapped':\", df_all_rankings_score['score_mapped'].isna().sum())\n",
    "print(\"NaN values in 'qrels_score_mapped':\", df_all_rankings_score['qrels_score_mapped'].isna().sum())\n",
    "\n",
    "# Remove rows with NaN values to ensure clean data for analysis\n",
    "df_clean = df_all_rankings_score.dropna(subset=['score_mapped', 'qrels_score_mapped'])\n",
    "\n",
    "# Generate the confusion matrix and performance metrics\n",
    "try:\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(df_clean['score_mapped'], df_clean['qrels_score_mapped'], labels=labels)\n",
    "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "    # Calculate F1 scores for each class\n",
    "    f1_scores = f1_score(df_clean['score_mapped'], df_clean['qrels_score_mapped'], average=None, labels=labels)\n",
    "    f1_df = pd.DataFrame({'Score Category': labels, 'F1 Score': f1_scores})\n",
    "\n",
    "    # Plot confusion matrix and F1 scores\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), gridspec_kw={'width_ratios': [3, 1]})\n",
    "\n",
    "    # Confusion Matrix plot\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
    "    ax1.set_title('Confusion Matrix: Actual vs. Predicted Scores')\n",
    "    ax1.set_ylabel('Actual Score')\n",
    "    ax1.set_xlabel('Predicted qrels_score')\n",
    "\n",
    "    # F1 Scores plot\n",
    "    sns.barplot(x='F1 Score', y='Score Category', data=f1_df, ax=ax2, order=labels)\n",
    "    ax2.set_title('F1 Scores by Score Category')\n",
    "    ax2.set_xlabel('F1 Score')\n",
    "    ax2.set_ylabel('Score Category')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print detailed classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(df_clean['score_mapped'], df_clean['qrels_score_mapped']))\n",
    "\n",
    "    # Calculate and print overall weighted F1 score\n",
    "    overall_f1 = f1_score(df_clean['score_mapped'], df_clean['qrels_score_mapped'], average='weighted')\n",
    "    print(f\"\\nOverall F1 Score (weighted): {overall_f1:.4f}\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Please check the unique values in both columns and ensure they match the expected labels.\")\n",
    "\n",
    "# Print label distribution for both actual and predicted scores\n",
    "print(\"\\nActual Score Distribution:\")\n",
    "print(df_clean['score_mapped'].value_counts(normalize=True))\n",
    "print(\"\\nPredicted Score Distribution:\")\n",
    "print(df_clean['qrels_score_mapped'].value_counts(normalize=True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. Data Preparation:\n",
    "#    - Converts scores to a consistent string format and maps them to meaningful categories.\n",
    "#    - Ensures data cleanliness by removing any rows with NaN values.\n",
    "\n",
    "# 2. Comprehensive Evaluation:\n",
    "#    - Generates a confusion matrix to visualize the model's performance across all categories.\n",
    "#    - Calculates F1 scores for each category, providing a balanced measure of precision and recall.\n",
    "\n",
    "# 3. Visualization:\n",
    "#    - Creates intuitive visualizations of the confusion matrix and F1 scores, making it easy to identify strengths and weaknesses in the model's performance.\n",
    "\n",
    "# 4. Detailed Reporting:\n",
    "#    - Prints a comprehensive classification report, including precision, recall, and F1-score for each category.\n",
    "#    - Calculates an overall weighted F1 score, giving a single metric for overall performance.\n",
    "\n",
    "# 5. Error Handling:\n",
    "#    - Includes try-except block to catch and report any issues with label mismatches or unexpected values.\n",
    "\n",
    "# 6. Distribution Analysis:\n",
    "#    - Compares the distribution of actual vs. predicted scores, which can reveal biases in the model's predictions.\n",
    "\n",
    "# This analysis provides a thorough evaluation of your clinical trial eligibility assessment model:\n",
    "# - It allows you to see where the model performs well and where it struggles.\n",
    "# - The confusion matrix helps identify common misclassifications.\n",
    "# - F1 scores provide a balanced view of performance for each category.\n",
    "# - The overall F1 score gives a single metric to track improvements over time.\n",
    "# - Comparing actual and predicted distributions can reveal systemic biases in the model.\n",
    "\n",
    "\n",
    "# 1. Classification Report:\n",
    "\n",
    "#    Eligible:\n",
    "#    - Precision: 0.58 (58% of patients predicted as eligible were actually eligible)\n",
    "#    - Recall: 0.51 (51% of actually eligible patients were correctly identified)\n",
    "#    - F1-score: 0.54 (harmonic mean of precision and recall)\n",
    "\n",
    "#    Excluded:\n",
    "#    - Precision: 0.59 (59% of patients predicted as excluded were actually excluded)\n",
    "#    - Recall: 0.37 (only 37% of actually excluded patients were correctly identified)\n",
    "#    - F1-score: 0.45 (lower than Eligible, indicating poorer performance)\n",
    "\n",
    "#    Not Relevant:\n",
    "#    - All zeros, indicating that the model never predicted this category or there were no actual cases in this category\n",
    "\n",
    "# 2. Overall Metrics:\n",
    "#    - Accuracy: 0.44 (the model is correct only 44% of the time)\n",
    "#    - Weighted F1 Score: 0.4969 (less than 0.5, indicating poor overall performance)\n",
    "\n",
    "# 3. Distribution Comparison:\n",
    "#    Actual Distribution:\n",
    "#    - Excluded: 52.4%\n",
    "#    - Eligible: 47.6%\n",
    "#    - Not Relevant: 0%\n",
    "\n",
    "#    Predicted Distribution:\n",
    "#    - Eligible: 41.7%\n",
    "#    - Excluded: 33.0%\n",
    "#    - Not Relevant: 25.2%\n",
    "\n",
    "# Interpretation:\n",
    "# 1. Poor Overall Performance: The weighted F1 score of 0.4969 is quite low, suggesting that the model's performance is barely better than random guessing (which would be 0.33 for a three-class problem).\n",
    "# 2. Imbalanced Predictions: The model is overpredicting \"Not Relevant\" cases (25.2%) when there are actually none in the true labels. This indicates a significant misalignment between the model's understanding and the actual data.\n",
    "# 3. Low Recall for Excluded: The model is particularly bad at identifying excluded patients, catching only 37% of them. This could lead to inappropriately including patients in trials they should be excluded from, which is a serious issue in clinical settings.\n",
    "# 4. Absence of \"Not Relevant\" in Actual Data: The actual data doesn't contain any \"Not Relevant\" cases, which is unusual and might indicate an issue with the dataset or the labeling process.\n",
    "# 6. Clinical Implications: The poor performance, especially in distinguishing between eligible and excluded patients, could lead to serious issues in real-world application, potentially putting patients at risk or compromising trial integrity."
   ]
  },
  {
   "cell_type": "raw",
   "id": "52d9f288-2ce2-44bb-8a72-1016c4c8a244",
   "metadata": {},
   "source": [
    "# new input here for classic "
   ]
  },
  {
   "cell_type": "raw",
   "id": "eabc9275-098c-46ee-9035-7de863b6b7b2",
   "metadata": {},
   "source": [
    "# results_to_workwith = 'TrialGPT_working_wtf/TrialGPT/results_gpt4o' #TrialGPT_working_wtf/TrialGPT/results_gpt4o\n",
    "# results_to_workwith = 'TrialGPT_working_wtf/TrialGPT/results'\n",
    "# results_to_workwith = 'TrialGPT/results'\n",
    "results_to_workwith = 'results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b31cc5d-1563-470d-b48c-903040d6eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the JSON file containing matching results\n",
    "json_files = glob.glob(os.path.join(results_to_workwith, 'matching_results_*.json'))\n",
    "\n",
    "# Raise an error if no matching file is found\n",
    "if not json_files:\n",
    "    raise FileNotFoundError(f\"No matching JSON file found in {results_to_workwith}\")\n",
    "\n",
    "# Use the first file found (assuming there's only one matching file)\n",
    "json_file = json_files[0]\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Prepare a list to store the flattened data\n",
    "rows = []\n",
    "\n",
    "# Iterate through each patient in the data\n",
    "for patient_id, patient_data in data.items():\n",
    "    # Iterate through each trial for this patient\n",
    "    for trial_id, trial_data in patient_data.items():\n",
    "        # Iterate through each trial's data (NCT IDs)\n",
    "        for nct_id, nct_data in trial_data.items():\n",
    "            # Process both inclusion and exclusion criteria\n",
    "            for inc_exc in [\"inclusion\", \"exclusion\"]:\n",
    "                for criterion_number, criterion_info in nct_data[inc_exc].items():\n",
    "                    # Append flattened data to rows list\n",
    "                    rows.append({\n",
    "                        'patient_id': patient_id,\n",
    "                        'TODOremove': trial_id,  # Note: This column might need to be removed later\n",
    "                        'trial_id': nct_id,\n",
    "                        'criterion_type': inc_exc,\n",
    "                        'criterion_number': criterion_number,\n",
    "                        'brief_reasoning': criterion_info[0],\n",
    "                        'sentence_ids': criterion_info[1],\n",
    "                        'eligibility_label': criterion_info[2]\n",
    "                    })\n",
    "\n",
    "# Create a DataFrame from the flattened data\n",
    "df_matching_results = pd.DataFrame(rows)\n",
    "\n",
    "# Display basic information about the DataFrame\n",
    "print(df_matching_results.info())\n",
    "\n",
    "# Optional: Display the first few rows of the DataFrame\n",
    "# print(df_matching_results.head())\n",
    "\n",
    "# Optional: Save to CSV\n",
    "# df_matching_results.to_csv('flattened_matching_results.csv', index=False)\n",
    "# print(\"DataFrame saved to 'flattened_matching_results.csv'\")\n",
    "\n",
    "# Display distribution of eligibility labels\n",
    "print(\"\\nDistribution of eligibility labels:\")\n",
    "print(df_matching_results['eligibility_label'].value_counts(normalize=True))\n",
    "\n",
    "# Display the number of unique patients and trials\n",
    "print(f\"\\nNumber of unique patients: {df_matching_results['patient_id'].nunique()}\")\n",
    "print(f\"Number of unique trials: {df_matching_results['trial_id'].nunique()}\")\n",
    "\n",
    "# Calculate and display the average number of criteria per trial\n",
    "criteria_per_trial = df_matching_results.groupby('trial_id').size()\n",
    "print(f\"\\nAverage number of criteria per trial: {criteria_per_trial.mean():.2f}\")\n",
    "\n",
    "# Display distribution of inclusion vs exclusion criteria\n",
    "print(\"\\nDistribution of inclusion vs exclusion criteria:\")\n",
    "print(df_matching_results['criterion_type'].value_counts(normalize=True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. Data Loading: It dynamically finds and loads the JSON file containing detailed matching results, which is essential for in-depth analysis of the model's performance.\n",
    "# 2. Data Flattening: The nested JSON structure is flattened into a tabular format, making it easier to analyze using pandas.\n",
    "# 3. Comprehensive Data Capture: For each patient-trial-criterion combination, it captures detailed information including the reasoning and eligibility label.\n",
    "# 4. Flexibility: The code can handle multiple trials per patient and both inclusion and exclusion criteria.\n",
    "# 5. Data Integrity Check: It provides summary statistics that help verify the integrity and structure of the data (e.g., number of unique patients and trials).\n",
    "# 6. Performance Insights: The distribution of eligibility labels gives a quick overview of the model's decisions.\n",
    "# 7. Dataset Characteristics: Information like average criteria per trial and distribution of inclusion vs exclusion criteria provides insights into the complexity of the eligibility assessment task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4779b9-6dfd-4925-83db-ef99dc922d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matching_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3152ee4e-a6eb-4936-8368-b6bce2fabda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_criteria(criteria: str) -> str:\n",
    "    \"\"\"\n",
    "    Parse and format a specific set of clinical trial criteria (either inclusion or exclusion).\n",
    "    This function cleans and numbers each criterion.\n",
    "    \"\"\"\n",
    "    output = \"\"\n",
    "    criteria_lines = criteria.split(\"\\n\")\n",
    "    idx = 0\n",
    "    for line in criteria_lines:\n",
    "        line = line.strip()\n",
    "        # Skip lines that are headers or too short\n",
    "        if \"inclusion criteria\" in line.lower() or \"exclusion criteria\" in line.lower():\n",
    "            continue\n",
    "        if len(line) < 5:\n",
    "            continue\n",
    "        # Add numbered criterion to output\n",
    "        output += f\"{idx}. {line}\\n\"\n",
    "        idx += 1\n",
    "    return output\n",
    "\n",
    "# Set the path to the corpus file\n",
    "corpus_file = 'dataset/sigir/corpus.jsonl'\n",
    "\n",
    "# Prepare a list to store the flattened data\n",
    "rows = []\n",
    "\n",
    "# Read and process the JSONL file\n",
    "with open(corpus_file, 'r') as f:\n",
    "    for line in f:\n",
    "        trial_data = json.loads(line)\n",
    "        trial_id = trial_data['_id']\n",
    "        \n",
    "        # Process inclusion criteria\n",
    "        inclusion_criteria = parse_criteria(trial_data['metadata']['inclusion_criteria'])\n",
    "        for idx, criterion in enumerate(inclusion_criteria.strip().split('\\n')):\n",
    "            if criterion:  # Skip empty lines\n",
    "                criterion_number, criterion_text = criterion.split('. ', 1)\n",
    "                rows.append({\n",
    "                    'trial_id': trial_id,\n",
    "                    'criterion_type': 'inclusion',\n",
    "                    'criterion_number': criterion_number,\n",
    "                    'criterion_text': criterion_text\n",
    "                })\n",
    "        \n",
    "        # Process exclusion criteria\n",
    "        exclusion_criteria = parse_criteria(trial_data['metadata']['exclusion_criteria'])\n",
    "        for idx, criterion in enumerate(exclusion_criteria.strip().split('\\n')):\n",
    "            if criterion:  # Skip empty lines\n",
    "                criterion_number, criterion_text = criterion.split('. ', 1)\n",
    "                rows.append({\n",
    "                    'trial_id': trial_id,\n",
    "                    'criterion_type': 'exclusion',\n",
    "                    'criterion_number': criterion_number,\n",
    "                    'criterion_text': criterion_text\n",
    "                })\n",
    "\n",
    "# Create a DataFrame from the flattened data\n",
    "df_corpus = pd.DataFrame(rows)\n",
    "\n",
    "# Display basic information about the DataFrame\n",
    "print(df_corpus.info())\n",
    "\n",
    "# Optional: Display the first few rows of the DataFrame\n",
    "# print(df_corpus.head())\n",
    "\n",
    "# Display the number of unique trials\n",
    "print(f\"\\nNumber of unique trials: {df_corpus['trial_id'].nunique()}\")\n",
    "\n",
    "# Calculate and display the average number of criteria per trial\n",
    "criteria_per_trial = df_corpus.groupby('trial_id').size()\n",
    "print(f\"\\nAverage number of criteria per trial: {criteria_per_trial.mean():.2f}\")\n",
    "\n",
    "# Display distribution of inclusion vs exclusion criteria\n",
    "print(\"\\nDistribution of inclusion vs exclusion criteria:\")\n",
    "print(df_corpus['criterion_type'].value_counts(normalize=True))\n",
    "\n",
    "# Optional: Save to CSV\n",
    "# df_corpus.to_csv('corpus_criteria.csv', index=False)\n",
    "# print(\"DataFrame saved to 'corpus_criteria.csv'\")\n",
    "\n",
    "\n",
    "# 1. Data Extraction: It extracts and structures the inclusion and exclusion criteria from the clinical trial corpus, which is essential for detailed analysis of trial eligibility requirements.\n",
    "# 2. Data Cleaning: The `parse_criteria` function cleans and formats the criteria text, removing headers and short lines, and adding consistent numbering.\n",
    "# 3. Standardization: By processing both inclusion and exclusion criteria in the same way, it creates a standardized format for all criteria across different trials.\n",
    "# 4. Granular Data Structure: Each criterion is stored as a separate row, allowing for detailed analysis at the individual criterion level.\n",
    "# 5. Data Integrity: The code maintains the association between criteria and their respective trials through the 'trial_id' field.\n",
    "# 6. Comprehensive Coverage: It processes both inclusion and exclusion criteria, providing a complete picture of each trial's eligibility requirements.\n",
    "# 7. Dataset Insights: The summary statistics (number of trials, average criteria per trial, distribution of inclusion vs exclusion criteria) provide valuable insights into the structure and complexity of the trial corpus.\n",
    "\n",
    "# The resulting `df_corpus` DataFrame serves as a comprehensive resource for understanding the landscape of clinical trial eligibility criteria in your dataset. This structured data is essential for developing and evaluating automated systems for trial matching and eligibility assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770f598-d819-4193-b44f-3a0b5d627711",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7efa96c-f819-4c59-b378-aeba2cda7b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a left join between df_matching_results and df_corpus\n",
    "# df_matching_results is the output of the model prediction \n",
    "# This combines the model's predictions with the actual criterion text\n",
    "df_joined = pd.merge(df_matching_results, \n",
    "                     df_corpus[['trial_id', 'criterion_type', 'criterion_number', 'criterion_text']], \n",
    "                     on=['trial_id', 'criterion_type', 'criterion_number'], \n",
    "                     how='left')\n",
    "\n",
    "# Display basic information about the joined DataFrame\n",
    "print(df_joined.info())\n",
    "\n",
    "# Optional: Display the first few rows of the joined DataFrame\n",
    "# print(df_joined.head())\n",
    "\n",
    "# Check for any unmatched rows (rows where criterion_text is NaN)\n",
    "unmatched = df_joined[df_joined['criterion_text'].isna()]\n",
    "print(f\"\\nNumber of rows without matching criterion_text: {len(unmatched)}\")\n",
    "\n",
    "# Calculate the percentage of matched rows\n",
    "total_rows = len(df_joined)\n",
    "matched_rows = total_rows - len(unmatched)\n",
    "match_percentage = (matched_rows / total_rows) * 100\n",
    "\n",
    "# Print matching statistics\n",
    "print(f\"\\nTotal rows: {total_rows}\")\n",
    "print(f\"Matched rows: {matched_rows}\")\n",
    "print(f\"Match percentage: {match_percentage:.2f}%\")\n",
    "\n",
    "# Display a sample of unmatched rows if any exist\n",
    "if len(unmatched) > 0:\n",
    "    print(\"\\nSample of unmatched rows:\")\n",
    "    print(unmatched[['trial_id', 'criterion_type', 'criterion_number', 'brief_reasoning']].head())\n",
    "\n",
    "# Rename 'eligibility_label' to 'eligibility_label_predicted' for clarity\n",
    "df_joined = df_joined.rename(columns={'eligibility_label': 'eligibility_label_predicted'})\n",
    "\n",
    "# Optional: Save the joined DataFrame to CSV\n",
    "# df_joined.to_csv('matching_results_with_criterion_text.csv', index=False)\n",
    "# print(\"\\nJoined DataFrame saved to 'matching_results_with_criterion_text.csv'\")\n",
    "\n",
    "\n",
    "# 1. Data Integration: It combines the model's predictions (df_matching_results) with the actual criterion text (df_corpus), creating a comprehensive dataset for analysis.\n",
    "# 2. Completeness Check: By using a left join, it ensures that all rows from df_matching_results are retained, even if there's no matching criterion text in df_corpus.\n",
    "# 3. Data Quality Assessment: The code checks for unmatched rows, which could indicate discrepancies between the model's input and the corpus data.\n",
    "# 4. Match Rate Calculation: It calculates and displays the percentage of successfully matched rows, providing a quick measure of data alignment.\n",
    "# 5. Error Analysis: By displaying samples of unmatched rows, it allows for investigation of specific cases where data misalignment occurs.\n",
    "# 6. Clear Labeling: Renaming 'eligibility_label' to 'eligibility_label_predicted' clearly distinguishes the model's predictions from any ground truth labels.\n",
    "\n",
    "# This process is essential for:\n",
    "# - Validating the consistency between the model's input data and the corpus.\n",
    "# - Identifying any discrepancies that could affect the model's performance or the validity of the analysis.\n",
    "# - Preparing a comprehensive dataset that includes both the model's predictions and the full text of the criteria, enabling more detailed analysis.\n",
    "\n",
    "# The resulting df_joined DataFrame is a powerful tool for:\n",
    "# - Analyzing the model's performance in the context of specific criterion texts.\n",
    "# - Investigating cases where the model's predictions might be inconsistent with the actual criteria.\n",
    "# - Performing qualitative analysis of the model's reasoning (brief_reasoning) in relation to the full criterion text.\n",
    "\n",
    "# The high match percentage (assuming it's close to 100%) would indicate good alignment between the model's input data and the corpus, which is crucial for the validity of your analysis. Any unmatched rows should be carefully examined as they might represent edge cases or data inconsistencies that could affect the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc8c4c2-2383-4279-99e8-b5e3de4afeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2d81da-02d7-4bb3-975e-b90fe75d6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text by stripping whitespace and converting to lowercase\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    return str(text).strip().lower()\n",
    "\n",
    "# Clean the criterion_text in both DataFrames\n",
    "df_joined['criterion_text_clean'] = df_joined['criterion_text'].apply(clean_text)\n",
    "df_final['criterion_text_clean'] = df_final['criterion_text'].apply(clean_text)\n",
    "\n",
    "# Count unique patient-trial combinations before merging\n",
    "patient_trials_in_joined = df_joined.groupby(['patient_id', 'trial_id']).size().reset_index(name='count')\n",
    "patient_trials_in_final = df_final.groupby(['patient_id', 'trial_id']).size().reset_index(name='count')\n",
    "\n",
    "print(\"Before merging:\")\n",
    "print(f\"Unique patient-trial combinations in df_joined: {len(patient_trials_in_joined)}\")\n",
    "print(f\"Unique patient-trial combinations in df_final: {len(patient_trials_in_final)}\")\n",
    "\n",
    "# Perform the inner join with cleaned text\n",
    "df_merged = pd.merge(df_joined, \n",
    "                     df_final, \n",
    "                     on=['patient_id', 'trial_id', 'criterion_type', 'criterion_text_clean'], \n",
    "                     how='inner',\n",
    "                     suffixes=('', '_final'))\n",
    "\n",
    "# Count unique patient-trial combinations after merging\n",
    "patient_trials_after_merge = df_merged.groupby(['patient_id', 'trial_id']).size().reset_index(name='count')\n",
    "\n",
    "print(\"\\nAfter merging:\")\n",
    "print(f\"Unique patient-trial combinations in merged DataFrame: {len(patient_trials_after_merge)}\")\n",
    "\n",
    "# Calculate the number and percentage of rows kept after the merge\n",
    "rows_kept = len(df_merged)\n",
    "rows_original = len(df_joined)\n",
    "percentage_kept = (rows_kept / rows_original) * 100\n",
    "\n",
    "print(f\"\\nTotal rows in original df_joined: {rows_original}\")\n",
    "print(f\"Rows kept after inner join: {rows_kept}\")\n",
    "print(f\"Percentage of rows kept: {percentage_kept:.2f}%\")\n",
    "\n",
    "# Calculate percentage of patient-trial combinations kept\n",
    "percentage_combinations_kept = (len(patient_trials_after_merge) / len(patient_trials_in_joined)) * 100\n",
    "print(f\"\\nPercentage of patient-trial combinations kept: {percentage_combinations_kept:.2f}%\")\n",
    "\n",
    "# Find patient-trial combinations that were lost in the merge\n",
    "combinations_lost = set(zip(patient_trials_in_joined['patient_id'], patient_trials_in_joined['trial_id'])) - \\\n",
    "                    set(zip(patient_trials_after_merge['patient_id'], patient_trials_after_merge['trial_id']))\n",
    "print(f\"\\nNumber of patient-trial combinations lost in merge: {len(combinations_lost)}\")\n",
    "\n",
    "if combinations_lost:\n",
    "    print(\"Sample of patient-trial combinations lost (up to 10):\")\n",
    "    print(list(combinations_lost)[:10])\n",
    "\n",
    "# Analyze why patient-trial combinations were lost\n",
    "if combinations_lost:\n",
    "    print(\"\\nAnalyzing why patient-trial combinations were lost:\")\n",
    "    for patient_id, trial_id in list(combinations_lost)[:5]:  # Analyze first 5 lost combinations\n",
    "        joined_criteria = df_joined[(df_joined['patient_id'] == patient_id) & (df_joined['trial_id'] == trial_id)]\n",
    "        final_criteria = df_final[(df_final['patient_id'] == patient_id) & (df_final['trial_id'] == trial_id)]\n",
    "        \n",
    "        print(f\"\\nPatient ID: {patient_id}, Trial ID: {trial_id}\")\n",
    "        print(f\"Criteria in df_joined: {len(joined_criteria)}\")\n",
    "        print(f\"Criteria in df_final: {len(final_criteria)}\")\n",
    "        \n",
    "        if len(final_criteria) == 0:\n",
    "            print(\"This patient-trial combination does not exist in df_final\")\n",
    "        else:\n",
    "            merged_criteria = pd.merge(joined_criteria, final_criteria, \n",
    "                                       on=['patient_id', 'trial_id', 'criterion_type', 'criterion_text_clean'],\n",
    "                                       how='inner')\n",
    "            print(f\"Matching criteria: {len(merged_criteria)}\")\n",
    "            \n",
    "            if len(merged_criteria) == 0:\n",
    "                print(\"Sample of non-matching criteria:\")\n",
    "                print(\"From df_joined:\")\n",
    "                print(joined_criteria['criterion_text_clean'].head())\n",
    "                print(\"From df_final:\")\n",
    "                print(final_criteria['criterion_text_clean'].head())\n",
    "\n",
    "\n",
    "# 1. Data Cleaning: It standardizes the criterion text across both dataframes, ensuring consistent comparison.\n",
    "# 2. Merge Quality Assessment: It thoroughly analyzes the merge operation between df_joined and df_final, providing detailed statistics on data retention and loss.\n",
    "# 3. Combination Tracking: By tracking patient-trial combinations before and after the merge, it helps identify any specific cases that might be lost in the process.\n",
    "# 4. Data Loss Investigation: For combinations that didn't make it through the merge, it provides a detailed analysis to understand why, which is crucial for data integrity.\n",
    "# 5. Merge Validation: The code verifies that the merge operation maintains the integrity of the data and identifies any discrepancies.\n",
    "                \n",
    "# Key aspects of this process:\n",
    "# - Text Cleaning: Ensures that minor differences in text formatting don't cause mismatches.\n",
    "# - Inner Join: Only keeps rows where there's a match in both dataframes, which could lead to data loss but ensures high-quality matches.\n",
    "# - Detailed Statistics: Provides percentages of rows and combinations kept, giving a clear picture of data retention.\n",
    "# - Loss Analysis: Investigates specific cases of data loss, which is crucial for understanding any systematic issues in the data or merge process.\n",
    "\n",
    "# This thorough approach to data merging and validation is essential for:\n",
    "# - Ensuring the reliability of your final dataset.\n",
    "# - Identifying any systematic issues in data collection or processing.\n",
    "# - Providing a clear understanding of what data is included in your final analysis.\n",
    "# - Enabling informed decisions about how to handle any data discrepancies or losses.\n",
    "\n",
    "\n",
    "# These results provide valuable insights into the data merging process and the quality of your datasets. Let's break down what these results mean:\n",
    "\n",
    "# 1. Overall Retention:\n",
    "#    - 83.49% of rows were kept after the merge, which is a good retention rate.\n",
    "#    - 96.12% of patient-trial combinations were retained, which is excellent.\n",
    "\n",
    "# 2. Data Loss:\n",
    "#    - Only 4 out of 103 patient-trial combinations were lost (3.88% loss).\n",
    "#    - This small loss suggests generally good alignment between df_joined and df_final.\n",
    "\n",
    "# 3. Specific Cases of Data Loss:\n",
    "#    The analysis of lost combinations reveals interesting patterns:\n",
    "\n",
    "#    a) Criterion Splitting:\n",
    "#       - In most cases, criteria in df_final are split into separate rows, while in df_joined they are combined.\n",
    "#       - Example: For 'sigir-201422', df_joined has 2 criteria, while df_final has 5.\n",
    "\n",
    "#    b) Text Formatting:\n",
    "#       - Despite cleaning, there are still differences in how criteria are formatted.\n",
    "#       - Example: \"age 8-18 years\" vs. \"age 8-18 years, referred from emergency department...\"\n",
    "\n",
    "#    c) Consistent Pattern:\n",
    "#       - This splitting pattern is consistent across all analyzed lost combinations.\n",
    "\n",
    "#    d) Missing Data:\n",
    "#       - One NaN value in df_joined for 'sigir-20154', which could contribute to the mismatch.\n",
    "\n",
    "# 4. Implications:\n",
    "#    a) Data Processing Differences:\n",
    "#       - There seems to be a systematic difference in how criteria are processed or stored between df_joined and df_final.\n",
    "#       - df_final consistently has more granular (split) criteria compared to df_joined.\n",
    "\n",
    "#    b) Text Matching Challenges:\n",
    "#       - Even with cleaning, exact text matching is challenging due to differences in criterion grouping.\n",
    "\n",
    "#    c) Potential for Improvement:\n",
    "#       - The consistent pattern suggests that adjusting the merging strategy could potentially recover these lost combinations.\n",
    "\n",
    "# 5. Recommendations:\n",
    "#    a) Criterion Splitting:\n",
    "#       - Consider splitting the criteria in df_joined to match the granularity in df_final before merging.\n",
    "\n",
    "#    b) Fuzzy Matching:\n",
    "#       - Implement a fuzzy matching algorithm to account for minor text differences.\n",
    "\n",
    "#    c) Manual Review:\n",
    "#       - Given the small number of lost combinations, a manual review and correction might be feasible.\n",
    "\n",
    "#    d) Standardization:\n",
    "#       - Develop a standardized process for criterion formatting to ensure consistency across datasets in future.\n",
    "\n",
    "# 6. Overall Assessment:\n",
    "#    - The merge process has been largely successful, retaining over 96% of patient-trial combinations.\n",
    "#    - The lost combinations appear to be due to systematic differences in data formatting rather than data quality issues.\n",
    "#    - With some refinement in the merging process, it's likely that even higher retention rates could be achieved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd270f6c-46f9-4084-96e1-97df52287f33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3118c17-640d-4036-a29f-1e7375965fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_eligibility(df, true_col, pred_col, title_prefix):\n",
    "    # Get distinct labels from both true and predicted columns\n",
    "    labels = sorted(set(df[true_col].unique()) | set(df[pred_col].unique()))\n",
    "\n",
    "    # Create a mapping of labels to numeric indices\n",
    "    label_to_index = {label: index for index, label in enumerate(labels)}\n",
    "\n",
    "    # Convert string labels to numeric indices for sklearn compatibility\n",
    "    y_true = df[true_col].map(label_to_index)\n",
    "    y_pred = df[pred_col].map(label_to_index)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Create a DataFrame from the confusion matrix for better visualization\n",
    "    cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "    # Calculate F1 scores for each class\n",
    "    f1_scores = f1_score(y_true, y_pred, average=None, labels=range(len(labels)))\n",
    "\n",
    "    # Create a DataFrame for F1 scores\n",
    "    f1_df = pd.DataFrame({'Eligibility': labels, 'F1 Score': f1_scores})\n",
    "\n",
    "    # Plot the confusion matrix and F1 scores\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8), gridspec_kw={'width_ratios': [3, 1]})\n",
    "\n",
    "    # Confusion Matrix plot\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', ax=ax1)\n",
    "    ax1.set_title(f'{title_prefix} Confusion Matrix: {true_col} vs. {pred_col}')\n",
    "    ax1.set_ylabel(true_col)\n",
    "    ax1.set_xlabel(pred_col)\n",
    "\n",
    "    # F1 Scores plot\n",
    "    sns.barplot(x='F1 Score', y='Eligibility', data=f1_df, ax=ax2)\n",
    "    ax2.set_title(f'{title_prefix} F1 Scores by Eligibility Class')\n",
    "    ax2.set_xlabel('F1 Score')\n",
    "    ax2.set_ylabel('Eligibility')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print detailed classification report\n",
    "    print(f\"\\n{title_prefix} Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=labels))\n",
    "\n",
    "    # Calculate and print overall weighted F1 score\n",
    "    overall_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(f\"\\n{title_prefix} Overall F1 Score (weighted): {overall_f1:.4f}\")\n",
    "\n",
    "    # Print label distribution for both true and predicted labels\n",
    "    print(f\"\\n{title_prefix} True Label Distribution:\")\n",
    "    print(df[true_col].value_counts(normalize=True))\n",
    "    print(f\"\\n{title_prefix} Prediction Distribution:\")\n",
    "    print(df[pred_col].value_counts(normalize=True))\n",
    "\n",
    "# Apply the analysis function to different datasets and comparisons\n",
    "\n",
    "# Compare GPT-4 predictions to expert labels in df_final\n",
    "analyze_eligibility(df_final, 'expert_eligibility', 'gpt4_eligibility', 'df_final')\n",
    "\n",
    "# Compare model predictions to expert labels in df_merged\n",
    "analyze_eligibility(df_merged, 'expert_eligibility', 'eligibility_label_predicted', 'df_merged (predicted)')\n",
    "\n",
    "# Compare GPT-4 predictions to expert labels in df_merged\n",
    "analyze_eligibility(df_merged, 'expert_eligibility', 'gpt4_eligibility', 'df_merged (GPT-4)')\n",
    "\n",
    "# This code is crucial for several reasons:\n",
    "# 1. Comprehensive Analysis: It provides a thorough evaluation of eligibility predictions, including confusion matrices, F1 scores, and label distributions.\n",
    "# 2. Visualization: The function creates clear visual representations of the confusion matrix and F1 scores, making it easier to interpret the results.\n",
    "# 3. Flexibility: The function is designed to work with different datasets and column names, allowing for easy comparison across different models or datasets.\n",
    "# 4. Detailed Metrics: It calculates and displays various performance metrics, giving a nuanced view of the model's strengths and weaknesses.\n",
    "# 5. Comparative Analysis: By applying the function to different datasets and prediction sources (GPT-4, model predictions), it enables direct comparison of performance across different approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d98779b-ff8f-4ec6-b22f-d05b52a6397c",
   "metadata": {},
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c892d5c0-fb04-43c6-9c78-5c5d6b859818",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```markdown\n",
    "## df_final TrialGPT-Criterion-Annotations Classification Report\n",
    "\n",
    "```\n",
    "                        precision    recall  f1-score   support\n",
    "\n",
    "              excluded       0.60      1.00      0.75         9\n",
    "              included       0.97      0.90      0.93       149\n",
    "        not applicable       0.48      0.98      0.65        55\n",
    "not enough information       0.88      0.88      0.88       285\n",
    "          not excluded       0.97      0.86      0.91       476\n",
    "          not included       0.72      0.78      0.75        23\n",
    "\n",
    "              accuracy                           0.88       997\n",
    "             macro avg       0.77      0.90      0.81       997\n",
    "          weighted avg       0.91      0.88      0.89       997\n",
    "```\n",
    "\n",
    "## Overall Performance\n",
    "\n",
    "df_final Overall F1 Score (weighted): 0.8870\n",
    "\n",
    "## True Label Distribution\n",
    "\n",
    "| expert_eligibility     | proportion |\n",
    "|------------------------|------------|\n",
    "| not excluded           | 0.477432   |\n",
    "| not enough information | 0.285858   |\n",
    "| included               | 0.149448   |\n",
    "| not applicable         | 0.055165   |\n",
    "| not included           | 0.023069   |\n",
    "| excluded               | 0.009027   |\n",
    "\n",
    "## Prediction Distribution\n",
    "\n",
    "| gpt4_eligibility       | proportion |\n",
    "|------------------------|------------|\n",
    "| not excluded           | 0.424273   |\n",
    "| not enough information | 0.284855   |\n",
    "| included               | 0.138415   |\n",
    "| not applicable         | 0.112337   |\n",
    "| not included           | 0.025075   |\n",
    "| excluded               | 0.015045   |\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Absolutely. This analysis is comparing the GPT-4-Turbo predictions (which were baked into the TrialGPT-Criterion-Annotations dataset) against the expert annotations in the same dataset. Let's break down what these results mean in detail:\n",
    "\n",
    "1. Dataset Context:\n",
    "   - This is analyzing df_final, which contains the TrialGPT-Criterion-Annotations data.\n",
    "   - The comparison is between 'expert_eligibility' (true labels) and 'gpt4_eligibility' (GPT-4-Turbo predictions that were included in the original dataset).\n",
    "\n",
    "2. Classification Report:\n",
    "\n",
    "   a) Excluded:\n",
    "      - Perfect recall (1.00) but lower precision (0.60), indicating GPT-4-Turbo correctly identified all truly excluded cases but also incorrectly labeled some non-excluded cases as excluded.\n",
    "      - Only 9 cases, so this high performance might not be representative.\n",
    "\n",
    "   b) Included:\n",
    "      - Very high precision (0.97) and recall (0.90), showing strong performance in identifying included cases.\n",
    "\n",
    "   c) Not Applicable:\n",
    "      - High recall (0.98) but low precision (0.48), suggesting GPT-4-Turbo overclassified cases as not applicable.\n",
    "\n",
    "   d) Not Enough Information:\n",
    "      - Balanced performance with both precision and recall at 0.88.\n",
    "\n",
    "   e) Not Excluded:\n",
    "      - High precision (0.97) but slightly lower recall (0.86), indicating some not excluded cases were misclassified.\n",
    "\n",
    "   f) Not Included:\n",
    "      - Moderate performance with precision at 0.72 and recall at 0.78.\n",
    "\n",
    "3. Overall Metrics:\n",
    "   - Accuracy: 0.88 (88% of all predictions were correct)\n",
    "   - Weighted F1 Score: 0.8870 (a strong overall performance)\n",
    "\n",
    "4. Label Distribution:\n",
    "   - True Distribution:\n",
    "     * 'not excluded' is the most common label (47.7%)\n",
    "     * 'excluded' is the least common (0.9%)\n",
    "   - Predicted Distribution:\n",
    "     * Generally aligns well with the true distribution\n",
    "     * Slight overestimation of 'not applicable' cases (11.2% predicted vs 5.5% true)\n",
    "\n",
    "5. Key Observations:\n",
    "   - The high overall F1 score (0.8870) suggests very strong performance by GPT-4-Turbo.\n",
    "   - Performance is particularly good for the most common classes ('not excluded', 'not enough information', 'included').\n",
    "   - There's a tendency to overpredict 'not applicable' cases.\n",
    "   - The model performs well even on less common classes like 'not included'.\n",
    "\n",
    "6. Important Considerations:\n",
    "   - These results are for GPT-4-Turbo predictions that were already part of the annotation set, which could potentially lead to an overly optimistic assessment.\n",
    "   - The close alignment between predicted and true distributions might be partly due to this integration of GPT-4-Turbo predictions into the dataset.\n",
    "   - The very high performance on the 'excluded' class, despite its rarity, is particularly noteworthy and might warrant further investigation.\n",
    "\n",
    "7. Potential Implications:\n",
    "   - While the performance appears excellent, it's crucial to remember that these GPT-4-Turbo predictions were already part of the dataset, which could inflate the apparent performance.\n",
    "   - This high performance sets a benchmark against which other models or versions can be compared.\n",
    "   - The slight discrepancies in label distribution provide insights into where GPT-4-Turbo might be biased or where human experts might disagree with the model.\n",
    "\n",
    "In conclusion, while these results show exceptional performance by GPT-4-Turbo, it's important to approach them with some caution given that the GPT-4-Turbo predictions were already integrated into the annotation set. This integration could lead to an artificially high alignment between predictions and expert labels. For a more robust evaluation, it would be valuable to test GPT-4-Turbo on a completely independent dataset or to compare these results with those from other models or annotation sources."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "499a1264-9b55-41ef-b61d-06fea22d4628",
   "metadata": {},
   "source": [
    "```markdown\n",
    "## df_merged (predicted) gpt-4o Classification Report:\n",
    "\n",
    "```\n",
    "                        precision    recall  f1-score   support\n",
    "\n",
    "              excluded       0.33      0.75      0.46         8\n",
    "              included       0.83      0.85      0.84       123\n",
    "        not applicable       0.77      0.51      0.61        53\n",
    "not enough information       0.46      0.71      0.56       254\n",
    "          not excluded       0.84      0.56      0.67       426\n",
    "          not included       0.23      0.38      0.29        16\n",
    "\n",
    "              accuracy                           0.64       880\n",
    "             macro avg       0.58      0.62      0.57       880\n",
    "          weighted avg       0.71      0.64      0.65       880\n",
    "```\n",
    "\n",
    "## Overall Performance\n",
    "\n",
    "df_merged (predicted) Overall F1 Score (weighted): 0.6503\n",
    "\n",
    "## True Label Distribution\n",
    "\n",
    "| expert_eligibility     | proportion |\n",
    "|------------------------|------------|\n",
    "| not excluded           | 0.484091   |\n",
    "| not enough information | 0.288636   |\n",
    "| included               | 0.139773   |\n",
    "| not applicable         | 0.060227   |\n",
    "| not included           | 0.018182   |\n",
    "| excluded               | 0.009091   |\n",
    "\n",
    "## Prediction Distribution\n",
    "\n",
    "| eligibility_label_predicted | proportion |\n",
    "|-----------------------------|------------|\n",
    "| not enough information      | 0.445455   |\n",
    "| not excluded                | 0.321591   |\n",
    "| included                    | 0.143182   |\n",
    "| not applicable              | 0.039773   |\n",
    "| not included                | 0.029545   |\n",
    "| excluded                    | 0.020455   |\n",
    "```\n",
    "\n",
    "\n",
    "Certainly. These results are from your custom GPT-4o model (released in May 2024), which is an improved version over GPT-4-Turbo (OpenAI announced GPT-4 Turbo in November 2023), along with enhanced prompting techniques you developed. Let's break down these results in detail:\n",
    "\n",
    "1. Dataset Context:\n",
    "   - This analysis is on df_merged, comparing 'expert_eligibility' (true labels) with 'eligibility_label_predicted' (predictions from your GPT-4o model).\n",
    "\n",
    "2. Classification Report:\n",
    "\n",
    "   a) Excluded:\n",
    "      - Low precision (0.33) but high recall (0.75), indicating the model tends to overpredict this category.\n",
    "      - Only 8 cases, so these metrics might not be very reliable.\n",
    "\n",
    "   b) Included:\n",
    "      - Strong performance with precision at 0.83 and recall at 0.85.\n",
    "\n",
    "   c) Not Applicable:\n",
    "      - Good precision (0.77) but lower recall (0.51), suggesting the model misses some 'not applicable' cases.\n",
    "\n",
    "   d) Not Enough Information:\n",
    "      - Low precision (0.46) but higher recall (0.71), indicating a tendency to overuse this label.\n",
    "\n",
    "   e) Not Excluded:\n",
    "      - High precision (0.84) but lower recall (0.56), suggesting the model misses many 'not excluded' cases.\n",
    "\n",
    "   f) Not Included:\n",
    "      - Poor performance with low precision (0.23) and recall (0.38).\n",
    "\n",
    "3. Overall Metrics:\n",
    "   - Accuracy: 0.64 (64% of all predictions were correct)\n",
    "   - Weighted F1 Score: 0.6503\n",
    "\n",
    "4. Label Distribution:\n",
    "   - True Distribution:\n",
    "     * Similar to the previous analysis, with 'not excluded' being most common (48.4%)\n",
    "   - Predicted Distribution:\n",
    "     * Overestimates 'not enough information' (44.5% predicted vs 28.9% true)\n",
    "     * Underestimates 'not excluded' (32.2% predicted vs 48.4% true)\n",
    "\n",
    "5. Key Observations:\n",
    "   - The overall performance (F1 score of 0.6503) is notably lower than the GPT-4-turbo results in the previous analysis.\n",
    "   - The model performs best on 'included' cases, which is crucial for clinical trial recruitment.\n",
    "   - There's a significant tendency to overpredict 'not enough information' cases.\n",
    "   - Performance on rare classes ('excluded', 'not included') is poor, likely due to class imbalance.\n",
    "\n",
    "6. Comparison to Previous Results:\n",
    "   - This model's performance is more modest compared to the GPT-4 results in the TrialGPT-Criterion-Annotations dataset.\n",
    "   - The distribution of predictions differs more from the true distribution, especially for 'not enough information' and 'not excluded' categories.\n",
    "\n",
    "7. Implications and Insights:\n",
    "   - The lower performance compared to the previous results suggests that:\n",
    "     a) The task is genuinely challenging, and the high performance in the annotation set might have been optimistic.\n",
    "     b) There might be differences in how your model and the original GPT-4 approach the task.\n",
    "   - The tendency to predict 'not enough information' could indicate a more cautious approach by your model.\n",
    "   - The model struggles with less common categories, which is a common challenge in imbalanced datasets.\n",
    "\n",
    "8. Potential Areas for Improvement:\n",
    "   - Addressing class imbalance, particularly for 'excluded' and 'not included' categories.\n",
    "   - Refining the model's ability to distinguish between 'not enough information' and other categories.\n",
    "   - Improving recall for 'not excluded' cases, which form a large portion of the dataset.\n",
    "\n",
    "9. Strengths of Your Approach:\n",
    "   - Despite lower overall metrics, your model shows strong performance on 'included' cases, which is crucial for clinical trial recruitment.\n",
    "   - The more balanced performance across categories (compared to the extreme high performance on 'excluded' in the previous results) might indicate a more realistic and generalizable model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4b7028-d1a7-4e7f-8abc-f1edcfaa6c6d",
   "metadata": {},
   "source": [
    "Here's the properly formatted markdown for the provided data:\n",
    "\n",
    "```markdown\n",
    "## df_merged (predicted) llama70b Classification Report\n",
    "\n",
    "```\n",
    "                        precision    recall  f1-score   support\n",
    "\n",
    "              excluded       0.19      0.75      0.31         8\n",
    "              included       0.65      0.88      0.75       115\n",
    "        not applicable       0.54      0.27      0.36        52\n",
    "not enough information       0.65      0.38      0.48       224\n",
    "          not excluded       0.81      0.88      0.84       427\n",
    "          not included       0.14      0.31      0.20        16\n",
    "\n",
    "              accuracy                           0.70       842\n",
    "             macro avg       0.50      0.58      0.49       842\n",
    "          weighted avg       0.71      0.70      0.69       842\n",
    "```\n",
    "\n",
    "## Overall Performance\n",
    "\n",
    "df_merged (predicted) Overall F1 Score (weighted): 0.6869\n",
    "\n",
    "## True Label Distribution\n",
    "\n",
    "| expert_eligibility     | proportion |\n",
    "|------------------------|------------|\n",
    "| not excluded           | 0.507126   |\n",
    "| not enough information | 0.266033   |\n",
    "| included               | 0.136580   |\n",
    "| not applicable         | 0.061758   |\n",
    "| not included           | 0.019002   |\n",
    "| excluded               | 0.009501   |\n",
    "\n",
    "## Prediction Distribution\n",
    "\n",
    "| eligibility_label_predicted | proportion |\n",
    "|-----------------------------|------------|\n",
    "| not excluded                | 0.549881   |\n",
    "| included                    | 0.184086   |\n",
    "| not enough information      | 0.156770   |\n",
    "| not included                | 0.041568   |\n",
    "| excluded                    | 0.036817   |\n",
    "| not applicable              | 0.030879   |\n",
    "```\n",
    "\n",
    "Certainly! Let's analyze these results for the llama70b model in detail:\n",
    "\n",
    "Key observations and analysis:\n",
    "\n",
    "1. Overall Performance:\n",
    "   - Weighted F1 Score: 0.6869, which is better than your GPT-4o model (0.6503) but still significantly lower than the GPT-4 results (0.8905).\n",
    "\n",
    "2. Classification Report:\n",
    "   a) Excluded:\n",
    "      - Low precision (0.19) but high recall (0.75), indicating overclassification of this rare category.\n",
    "   b) Included:\n",
    "      - Good performance with precision 0.65 and high recall 0.88.\n",
    "   c) Not Applicable:\n",
    "      - Moderate precision (0.54) but low recall (0.27), suggesting underclassification.\n",
    "   d) Not Enough Information:\n",
    "      - Moderate precision (0.65) and low recall (0.38), indicating underuse of this label.\n",
    "   e) Not Excluded:\n",
    "      - Strong performance with high precision (0.81) and recall (0.88).\n",
    "   f) Not Included:\n",
    "      - Poor performance with very low precision (0.14) and low recall (0.31).\n",
    "\n",
    "3. Label Distribution:\n",
    "   - True Distribution: Dominated by 'not excluded' (50.7%) and 'not enough information' (26.6%).\n",
    "   - Predicted Distribution: Overpredicts 'not excluded' (55.0%) and underpredicts 'not enough information' (15.7%).\n",
    "\n",
    "4. Key Insights:\n",
    "   - The model performs best on the most common category ('not excluded') and struggles with rare categories.\n",
    "   - There's a tendency to overpredict 'not excluded' and underpredict 'not enough information'.\n",
    "   - Performance on 'included' cases is good, which is crucial for clinical trial recruitment.\n",
    "\n",
    "5. Comparison to Other Models:\n",
    "   - llama70b outperforms your GPT-4o model but falls short of the GPT-4 results in the original dataset.\n",
    "   - It shows a different error pattern compared to GPT-4o, with better performance on 'not excluded' but worse on 'not enough information'.\n",
    "\n",
    "6. Potential Areas for Improvement:\n",
    "   - Addressing class imbalance, especially for rare categories like 'excluded' and 'not included'.\n",
    "   - Improving the model's ability to identify 'not enough information' cases.\n",
    "   - Refining the classification of 'not applicable' cases to improve recall.\n",
    "\n",
    "7. Strengths of llama70b:\n",
    "   - Strong performance on the most common category ('not excluded').\n",
    "   - Good recall for 'included' cases, which is important for not missing potential trial participants.\n",
    "\n",
    "8. Limitations:\n",
    "   - Struggles with rare categories, which could lead to misclassifications in important edge cases.\n",
    "   - Tendency to overuse the 'not excluded' label, which might result in false positives.\n",
    "\n",
    "In conclusion, the llama70b model shows promising performance, particularly in its ability to handle the most common categories and identify included cases. However, it still faces challenges with rare categories and balancing between different labels. This performance suggests that while large language models like llama70b have strong potential in clinical trial eligibility assessment, there's still room for improvement, particularly in handling the nuances of less common cases and distinguishing between subtle categories like 'not enough information' and 'not excluded'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5d3677-5751-4b30-963d-daeb8bb5d1e2",
   "metadata": {},
   "source": [
    "\n",
    "```markdown\n",
    "## df_merged (GPT-4-turbo) Classification Report\n",
    "\n",
    "```\n",
    "                        precision    recall  f1-score   support\n",
    "\n",
    "              excluded       0.62      1.00      0.76         8\n",
    "              included       0.97      0.92      0.95       123\n",
    "        not applicable       0.51      0.98      0.67        53\n",
    "not enough information       0.89      0.88      0.88       254\n",
    "          not excluded       0.97      0.87      0.92       426\n",
    "          not included       0.65      0.69      0.67        16\n",
    "\n",
    "              accuracy                           0.88       880\n",
    "             macro avg       0.77      0.89      0.81       880\n",
    "          weighted avg       0.91      0.88      0.89       880\n",
    "```\n",
    "\n",
    "## Overall Performance\n",
    "\n",
    "df_merged (GPT-4) Overall F1 Score (weighted): 0.8905\n",
    "\n",
    "## True Label Distribution\n",
    "\n",
    "| expert_eligibility     | proportion |\n",
    "|------------------------|------------|\n",
    "| not excluded           | 0.484091   |\n",
    "| not enough information | 0.288636   |\n",
    "| included               | 0.139773   |\n",
    "| not applicable         | 0.060227   |\n",
    "| not included           | 0.018182   |\n",
    "| excluded               | 0.009091   |\n",
    "\n",
    "## Prediction Distribution\n",
    "\n",
    "| gpt4_eligibility       | proportion |\n",
    "|------------------------|------------|\n",
    "| not excluded           | 0.430682   |\n",
    "| not enough information | 0.287500   |\n",
    "| included               | 0.131818   |\n",
    "| not applicable         | 0.115909   |\n",
    "| not included           | 0.019318   |\n",
    "| excluded               | 0.014773   |\n",
    "```\n",
    "\n",
    "You've made an excellent observation here. Let's break this down and discuss its significance:\n",
    "\n",
    "1. Dataset Context:\n",
    "   - This analysis is on df_merged, comparing 'expert_eligibility' (true labels) with 'gpt4_eligibility' (GPT-4 predictions).\n",
    "   - Importantly, this is on the subset of 880 rows that remained after the inner join, down from the original 1054 rows.\n",
    "\n",
    "2. Performance Metrics:\n",
    "   - Overall F1 Score: 0.8905, which is slightly higher than the 0.8870 seen in the full df_final dataset.\n",
    "   - Accuracy: 0.88, matching the accuracy in the full dataset.\n",
    "\n",
    "3. Classification Report:\n",
    "   - The performance across all categories is very similar to what we saw in the full dataset.\n",
    "   - Notably high precision and recall for 'included', 'not excluded', and 'not enough information' categories.\n",
    "   - Perfect recall (1.00) for 'excluded', but with only 8 cases.\n",
    "\n",
    "4. Label Distributions:\n",
    "   - True label distribution is nearly identical to what we saw in the full dataset.\n",
    "   - Predicted distribution is also very close, with slight variations.\n",
    "\n",
    "5. Key Observations:\n",
    "   - Despite losing about 16.5% of the rows in the merging process, the performance metrics have slightly improved.\n",
    "   - The consistency in performance and distribution suggests that the lost rows were not significantly different in terms of difficulty or distribution from the retained rows.\n",
    "\n",
    "6. Implications:\n",
    "   - The improved performance on this subset indicates that the rows lost in the merge were not \"easy\" cases that artificially inflated the scores.\n",
    "   - If anything, the lost rows might have included some of the more challenging cases, as removing them led to a slight improvement in performance.\n",
    "\n",
    "7. Data Quality Insights:\n",
    "   - This result suggests that the data merging process, while reducing the dataset size, did not introduce a bias that favored easier cases.\n",
    "   - It provides confidence that the merged dataset is representative of the full dataset in terms of difficulty and distribution of cases.\n",
    "\n",
    "8. Comparison with Your Model:\n",
    "   - The stark difference in performance between this GPT-4 result (F1: 0.8905) and your GPT-4o model (F1: 0.6503) on the same subset of data becomes even more intriguing.\n",
    "   - It reinforces the notion that the high performance of GPT-4 in this dataset might be due to factors beyond just model capability, possibly including how it was integrated into the annotation process.\n",
    "\n",
    "9. Methodological Considerations:\n",
    "   - This result highlights the importance of careful data handling and analysis in machine learning projects.\n",
    "   - It demonstrates how data merging and filtering steps can impact results, and the importance of checking for unintended consequences of these processes.\n",
    "\n",
    "10. Future Directions:\n",
    "    - It might be worth investigating the characteristics of the ~174 rows that were lost in the merge. While they didn't negatively impact the GPT-4 performance, understanding why they were lost could provide insights into data quality or processing issues.\n",
    "    - The consistent high performance of GPT-4 across different subsets of the data suggests that it might be worthwhile to examine individual cases where your model disagrees with GPT-4, to understand the reasoning behind these differences.\n",
    "\n",
    "In conclusion, this analysis provides a valuable sanity check on your data processing steps and offers reassurance about the representativeness of your merged dataset. It also further emphasizes the exceptionally high performance of GPT-4 on this task, which contrasts sharply with your model's performance. This discrepancy continues to suggest that there might be some inherent bias or integration effect in how GPT-4's predictions were incorporated into the original dataset, making it a challenging benchmark to compare against directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea3b6e-d146-4bf1-8457-74eb3db2844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove specified columns from df_merged to create a slimmed-down version\n",
    "columns_to_drop = ['criterion_text_clean', 'text', 'score', 'trial_title', \n",
    "                   'criterion_text_final', 'num_trials', 'num_distinct_criteria']\n",
    "\n",
    "df_merged_slim = df_merged.drop(columns=columns_to_drop)\n",
    "\n",
    "# Create three comparison tables using the slimmed-down df_merged_slim\n",
    "\n",
    "# Table 1: Identify rows where the model's predictions differ from expert labels\n",
    "df_pred_vs_expert = df_merged_slim[df_merged_slim['eligibility_label_predicted'] != df_merged_slim['expert_eligibility']]\n",
    "\n",
    "# Table 2: Identify rows where the model's predictions differ from GPT-4 predictions\n",
    "df_pred_vs_gpt4 = df_merged_slim[df_merged_slim['eligibility_label_predicted'] != df_merged_slim['gpt4_eligibility']]\n",
    "\n",
    "# Table 3: Identify rows where expert labels differ from GPT-4 predictions\n",
    "df_expert_vs_gpt4 = df_merged_slim[df_merged_slim['expert_eligibility'] != df_merged_slim['gpt4_eligibility']]\n",
    "\n",
    "# Print summary information about the resulting dataframes\n",
    "print(f\"Columns in the slimmed-down dataframe: {df_merged_slim.columns.tolist()}\")\n",
    "print(f\"Rows where predicted != expert: {len(df_pred_vs_expert)}\")\n",
    "print(f\"Rows where predicted != GPT-4: {len(df_pred_vs_gpt4)}\")\n",
    "print(f\"Rows where expert != GPT-4: {len(df_expert_vs_gpt4)}\")\n",
    "\n",
    "# This final section of code is crucial for several reasons:\n",
    "\n",
    "# 1. Data Simplification:\n",
    "#    - It removes unnecessary columns, focusing the analysis on the most relevant information.\n",
    "#    - This simplification can improve processing speed and make the data easier to work with.\n",
    "\n",
    "# 2. Comparative Analysis:\n",
    "#    - It creates three distinct dataframes, each highlighting a different type of disagreement:\n",
    "#      a) Your model vs. expert labels\n",
    "#      b) Your model vs. GPT-4 predictions\n",
    "#      c) Expert labels vs. GPT-4 predictions\n",
    "#    - This allows for a nuanced understanding of where and how the different assessments diverge.\n",
    "\n",
    "# 3. Error Analysis Preparation:\n",
    "#    - By isolating the cases where there are disagreements, it sets the stage for detailed error analysis.\n",
    "#    - This is crucial for understanding the strengths and weaknesses of each approach (your model, GPT-4, and expert labeling).\n",
    "\n",
    "# 4. Quantification of Disagreements:\n",
    "#    - Printing the number of rows in each comparison dataframe provides a quick quantitative measure of how often each type of disagreement occurs.\n",
    "\n",
    "# 5. Model Evaluation:\n",
    "#    - Comparing your model's predictions against both expert labels and GPT-4 predictions allows for a multi-faceted evaluation of its performance.\n",
    "\n",
    "# 6. Expert vs. AI Comparison:\n",
    "#    - The comparison between expert labels and GPT-4 predictions (df_expert_vs_gpt4) is particularly interesting, as it highlights cases where human expertise and advanced AI disagree.\n",
    "\n",
    "# 7. Data Integrity Check:\n",
    "#    - This process can also serve as a final check on data integrity, ensuring that all expected columns are present and contain the anticipated types of data.\n",
    "\n",
    "# 8. Foundation for Further Analysis:\n",
    "#    - These comparison dataframes form the basis for more detailed investigations, such as:\n",
    "#      - Analyzing patterns in the types of criteria where disagreements occur\n",
    "#      - Identifying potential biases in any of the labeling methods\n",
    "#      - Pinpointing areas where your model might need improvement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dc1449-9199-42d9-87b2-eee71aed4762",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expert_vs_gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ade8f9-2114-48c0-af3f-ff303c9b939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_vs_expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d297f4-60e6-46f0-a2a2-a0d2f96f46c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_vs_gpt4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
